---
title: "Classification I"
---

## Summary

::: {style="text-align: justify;"}
This week, we explored a foundational concept in remote sensing: classification, the process of categorizing image pixels into distinct classes based on their spectral characteristics. The first part of the lecture covered real-world applications like tracking urban sprawl, linking air pollution to land use, and pinpointing deforestation hotspots. The forest monitoring and illegal logging case was really interesting. In that case, Landsat imagery was pre-processed—resampled, converted into top-of-atmosphere reflectance, and cleared of cloud cover—before introducing a key concept: "creating metrics". This essential step, performed before classification, involves deriving specific numerical measures or indices (e.g., maximum/minimum reflectance, mean values, or temporal slopes) from spectral bands across a time series. These metrics transform raw data into a "feature space," allowing algorithms to identify patterns, such as the detection of 5,000 deforestation sites in Brazil (Hansen et al., 2013).

The second part shifted focus to how classification is done, covering general machine learning algorithms (e.g., linear regression, random forest) and how they are applied in a spatial context. **Linear regression** is primarily utilized to estimate continuous measures, such as the distribution of atmospheric pollutants across geographic expanses, providing a foundational approach to predicting trends over space and time. **Random Forests**, conversely, assemble multiple decision trees that analyze random data samples to classify land types—forests, urban areas, or fields—enhancing accuracy through collective evaluation. This method leverages bootstrapping and random variable selection to enhance reliability, as demonstrated in its application to land cover mapping.

The lecture referenced Maximum Likelihood, an older probability-based method, but we did not cover it, as it is not used anymore. Looking at why, I learned how its reliance on assumptions of uniform data distribution is considered less effective for handling the diverse and complex imagery prevalent today, particularly given advancements in computational technology (Richards, 2006).

Despite these advances, challenges and limitations persist. **Decision trees risk overfitting**, where models excessively conform to training data, reducing their ability to generalize. Mitigation through pruning or setting minimum leaf sizes addresses this, though it increases complexity and requires precise tuning. Additionally, the lecture highlights a **trade-off between complexity and interpretability**: advanced classifiers like Random Forests and Support Vector Machines offer high accuracy but might become "black boxes," reducing interpretability.
:::

## Applications

::: {style="text-align: justify;"}
While the summary emphasized algorithmic approaches (e.g., Random Forests (RF), Support Vector Machines (SVM)), another critical distinction in classification methods we covered this week lies in **supervised** and **unsupervised** techniques, each offering unique applications. Unsupervised approaches, like k-means clustering, identify natural groupings without prior labels, offering flexibility for exploratory tasks where the goal is discovery rather than prediction. On the other hand, Supervised classification, where algorithms like Random Forests or SVM are trained on labeled datasets, excels in precise, goal-driven tasks. Both algorithms, are supervised learning methods, but they differ significantly in their approach, data handling, and performance characteristics. Here I’ll focus on highlighting that:

**- SVM:**

Similar to logistic regression, SVM minimizes generalization error without distribution assumptions, **separating classes** (e.g., forest vs. farmland) through an optimal hyperplane. It excels in high-dimensional applications, one example is detecting oil spills in synthetic aperture radar (SAR) imagery for environmental monitoring. Matkan et al. (2013) applied SVM to SAR images from the Gulf of Mexico, achieving high accuracy by training on texture and intensity features to differentiate oil-contaminated water from clean surfaces. Optimal performance required tuning parameters C (regularization) and gamma (kernel scale) through grid search and cross-validation, ensuring a balance between margin maximization and classification precision. **However**, this tuning process is computationally demanding, particularly for large datasets, and SVM’s limited interpretability—offering no direct insight into feature contributions—may restrict its use in applications requiring transparent decision-making, such as policy reporting for spill response strategies.

**- RF:**

This methods ability to **quantify feature importance** makes it valuable for various applications. One example is its role in analyzing burn severity drivers, as demonstrated by Huang et al. (2020) in California’s coastal mountains. Their study used RF to model burn severity, identifying slope, aspect, fuel moisture, and long-term climate as key predictors. During droughts, RF revealed that low fuel moisture and high climatic water deficit doubled high-severity burn areas. **However**, while efficient and interpretable, this approach may **oversimplify** localized fire behavior, particularly in heterogeneous landscapes where small differences in terrain or fuel types affect fire intensity. Additionally, correlated features (e.g., temperature and drought indices) can distort importance rankings, requiring careful variable selection.

<figure style=" text-align: center;">

<img src="images/supervised_learning.png" style="width: 70%;"/>

<figcaption style="text-align: center;">

Figure 1 - RF Vs SVM approach. <br> <a href="https://cambridgecoding.wordpress.com/2016/05/16/expanding-your-machine-learning-toolkit-randomized-search-computational-budgets-and-new-algorithms-2/">Source</a>

</figcaption>

</figure>
:::

## Reflections
::: {style="text-align: justify;"}


Reflecting on this lecture, I found it easy to follow and connect with, probably because classification appears in many familiar research areas like city planning or detecting change. Before this, I had some knowledge of machine learning, but not within remote sensing.

A few years ago, in 2020, I attempted to detect building footprints in Riyadh using an Esri machine learning tool. The results were inaccurate, and at the time, I assumed buildings shared similar traits like materials across regions. I didn’t fully understand how Riyadh’s buildings differ in spectral and structural characteristics from those in the tool’s training data and the big impact this has. Now, I see how methods like SVM classification, introduced in the lecture, could improve such work by adjusting to local details.

The lecture also raised a broader consideration: the tendency to utilize the most accurate tools without considering their suitability for a given problem. The lecture showed Maximum Likelihood isn’t used much anymore because it can’t keep up with today’s messy data (Richards, 2006), which makes me think: how often do we pick flashy tech over what actually works for the problem?
:::


## References

::: {style="text-align: left; font-size: 13px;"}
Hansen, M.C., Potapov, P.V., Moore, R., Hancher, M., Turubanova, S.A., Tyukavina, A., Thau, D., Stehman, S.V., Goetz, S.J., Loveland, T.R., Kommareddy, A., Egorov, A., Chini, L., Justice, C.O., Townshend, J.R.G., 2013. High-Resolution Global Maps of 21st-Century Forest Cover Change. Science 342, 850–853.

Huang, Y., Jin, Y., Schwartz, M. W., & Thorne, J. H. (2020). Intensified burn severity in California’s northern coastal mountains by drier climatic condition. Environmental Research Letters, 15(10), 104033-. https://doi.org/10.1088/1748-9326/aba6af

Matkan, A. A., Hajeb, M., & Azarakhsh, Z. (2013). Oil spill detection from SAR image using SVM based classification. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences., XL-1/W3, 55–60. https://doi.org/10.5194/isprsarchives-XL-1-W3-55-2013

Richards, J. A. (John A., & Jia, X. (2006). Remote sensing digital image analysis : an introduction. (4th ed. / John A. Richards, Xiuping Jia). Springer.
:::
