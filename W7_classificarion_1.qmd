---
title: "Classification I"
format: html
---

## Summary

::: {style="text-align: justify;"}
This week, we explored a foundational concept in remote sensing: classification, the process of categorizing image pixels into distinct classes based on their spectral characteristics. The first part of the lecture covered real-world applications like tracking urban sprawl, linking air pollution to land use, and pinpointing deforestation hotspots. The forest monitoring and illegal logging case was really interesting. In the case, Landsat imagery was pre-processed—resampled, converted into top-of-atmosphere reflectance, and cleared of cloud cover—before introducing a key concept: "creating metrics". This essential step, performed before classification, involves deriving specific numerical measures or indices (e.g., maximum/minimum reflectance, mean values, or temporal slopes) from spectral bands across a time series. These metrics transform raw data into a "feature space," allowing algorithms to identify patterns, such as the detection of 5,000 deforestation sites in Brazil (Hansen et al., 2013).

The second part shifted focus to how classification is done, covering general machine learning algorithms (e.g., linear regression, Random Forests) and how they are applied in a spatial context. **Linear regression** is primarily utilized to estimate continuous measures, such as the distribution of atmospheric pollutants across geographic expanses, providing a foundational approach to predicting trends over space and time. **Random Forests**, conversely, assemble multiple decision trees that analyze random data samples to classify land types—forests, urban areas, or fields—enhancing accuracy through collective evaluation. This method leverages bootstrapping and random variable selection to enhance reliability, as demonstrated in its application to land cover mapping.

The lecture referenced Maximum Likelihood, an older probability-based method, but we did not cover it, as it is used anymore. Looking at why, I learned how its reliance on assumptions of uniform data distribution is considered less effective for handling the diverse and complex imagery prevalent today, particularly given advancements in computational technology (Richards, 2006).

Despite these advances, challenges and limitations persist. **Decision trees risk overfitting**, where models excessively conform to training data, reducing their ability to generalize. Mitigation through pruning or setting minimum leaf sizes addresses this, though it increases complexity and requires precise tuning. Additionally, the lecture highlights a **trade-off between complexity and interpretability**: advanced classifiers like Random Forests and Support Vector Machines offer high accuracy but can become "black boxes," reducing interpretability.
:::

## Applications

::::: {style="text-align: justify;"}
While the summary focused on algorithmic approaches (e.g., Random Forests, SVM), another critical distinction we also covered in classification is **pixel-based** versus **object-based** methods. These approaches represent fundamentally different ways of interpreting remote sensing data, each with distinct advantages depending on the application.

**1. Pixel-Based Classification: Limitations in Urban Landscapes**

Pixel-based methods classify individual pixels based solely on their spectral signatures. While effective for large-scale studies like the global deforestation mapping by Hansen et al. (2013), which was discussed in the lecture, they face challenges in urban environments, including:

::: {style="padding-left: 10%;"}
-   Spectral ambiguity between different urban materials

-   Prevalence of mixed pixels in high-resolution imagery

-   Inability to incorporate spatial relationships between features
:::

**2. Object-Based Classification: A Spatial-Spectral Solution**

**Object-based image analysis** (OBIA) addresses these limitations through a hierarchical approach that:

::: {style="padding-left: 10%;"}
1.  Segments imagery into meaningful objects based on both spectral and spatial characteristics

2.  Incorporates contextual information through shape, texture, and relational metrics

3.  Utilizes multi-scale analysis to capture features at appropriate levels of detail
:::

The efficacy of this approach was demonstrated by Myint et al. (2011), whose comparative analysis of land cover in Phoenix showed a significant accuracy improvement—90.4% for object-based classification versus 67.6% for pixel-based classification, as visually evidenced in Figure 1 below. This finding underscores the effectiveness of incorporating spatial context in classification efforts.

![Figure 1 - (a) Test image; (b) output map produced by the object-based approach; (c) output map produced by the classical per-pixel classifier. Note: Cyan = buildings; orange = unmanaged soil; light green = grass; gray = other impervious surfaces, purple = swimming pools; dark green = trees and shrubs; and blue = lakes and ponds. - Source: (Myint et al., 2011)](images/OBIA_W6.png){fig-align="center"}
:::::

## Reflections

Reflecting on this lecture, I found it easy to follow and connect with, probably because classification appears in many familiar research areas like city planning or detecting change. Before this, I had some knowledge of machine learning, but not within remote sensing.

A few years ago, in 2020, I attempted to detect building footprints in Riyadh using an Esri machine learning tool. The results were inaccurate, and at the time, I assumed buildings shared similar traits like materials across regions. I didn’t fully understand how Riyadh’s buildings differ in spectral and structural characteristics from those in the tool’s training data and the big impact this has. Now, I see how methods like Support Vector Machines or object-based classification, introduced in the lecture, could improve such work by adjusting to local details.

The lecture also raised a broader consideration: the tendency to utilize the most accurate tools without considering their suitability for a given problem. The lecture showed Maximum Likelihood isn’t used much anymore because it can’t keep up with today’s messy data (Richards, 2006), which makes me think: how often do we pick flashy tech over what actually works for the problem?

The discussion of pixel-based versus object-based approaches stood out to me as well, especially as I’m planning to employ detection methodologies for my dissertation. I now understand how this fundamental choice shapes the patterns we can identify, so I’ll definitely dig deeper into this topic.

## References

::: {style="text-align: left; font-size: 13px;"}
Hansen, M.C., Potapov, P.V., Moore, R., Hancher, M., Turubanova, S.A., Tyukavina, A., Thau, D., Stehman, S.V., Goetz, S.J., Loveland, T.R., Kommareddy, A., Egorov, A., Chini, L., Justice, C.O., Townshend, J.R.G., 2013. High-Resolution Global Maps of 21st-Century Forest Cover Change. Science 342, 850–853.

Ma, L., Li, M., Ma, X., Cheng, L., Du, P., & Liu, Y. (2017). A review of supervised object-based land-cover image classification. ISPRS Journal of Photogrammetry and Remote Sensing, 130, 277–293. https://doi.org/10.1016/j.isprsjprs.2017.06.001

Myint, S. W., Gober, P., Brazel, A., Grossman-Clarke, S., & Weng, Q. (2011). Per-pixel vs. object-based classification of urban land cover extraction using high spatial resolution imagery. Remote Sensing of Environment, 115(5), 1145–1161. https://doi.org/10.1016/j.rse.2010.12.017

Richards, J. A. (John A., & Jia, X. (2006). Remote sensing digital image analysis : an introduction. (4th ed. / John A. Richards, Xiuping Jia). Springer.

Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., & Atkinson, P. M. (2019). Joint Deep Learning for land cover and land use classification. Remote Sensing of Environment, 221, 173–187. https://doi.org/10.1016/j.rse.2018.11.014

:::
