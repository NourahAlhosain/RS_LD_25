[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "W2_Xaringan.html",
    "href": "W2_Xaringan.html",
    "title": "2  Xaringan Tool",
    "section": "",
    "text": "2.1 Presentation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Xaringan Tool</span>"
    ]
  },
  {
    "objectID": "W2_Xaringan.html#reflections-on-xaringan",
    "href": "W2_Xaringan.html#reflections-on-xaringan",
    "title": "2  Xaringan Tool",
    "section": "2.2 Reflections on Xaringan",
    "text": "2.2 Reflections on Xaringan",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Xaringan Tool</span>"
    ]
  },
  {
    "objectID": "W3_Corrections.html",
    "href": "W3_Corrections.html",
    "title": "3  Corrections and Enhancements",
    "section": "",
    "text": "3.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "W3_Corrections.html#summary",
    "href": "W3_Corrections.html#summary",
    "title": "3  Corrections and Enhancements",
    "section": "",
    "text": "This week, the lecture covered the essentials of preparing and enhancing satellite imagery for analysis, splitting the content into two main parts. The first part focuses on corrections and its different categories: geometric correction aligns distorted images using Ground Control Points (we discussed how to pick these points, noting that features like vegetation are poor choices since they change over time, making them unreliable for ensuring spatial accuracy); atmospheric correction removes haze and scattering effects through methods like Dark Object Subtraction or radiative transfer models; topographic correction adjusts for terrain distortions using elevation models ; and radiometric correction converts Digital Numbers to spectral radiance with gain and bias. The second part covers joining datasets and enhancements: Mosaicking combines overlapping images using feathering to blend them smoothly, adjusting brightness so the edges match seamlessly. Enhancements include contrast stretching to amplify visibility, using band ratios to show things like vegetation more clearly, and applying Principal Component Analysis to simplify complicated data into essential components.\nWith that being said, it’s important to mention that remote sensing products now often come pre-corrected and enhanced. Yet, understanding these processes remains crucial, as it empowers users to evaluate data quality, address specific project needs, and adjust workflows across diverse satellite sources. As I was trying to understand Analysis Ready Data (ARD), this article helped me understand ARD’s core processing concepts. In addition, this week’s practical highlighted how each satellite offers unique products, and how collections—systematically organized groups of satellite imagery—are categorized into different processing levels, which indicate the degree of preprocessing applied to the data. For instance, Level 1 data includes raw or minimally processed imagery, while higher levels, like Level 3, provide specialized outputs such as vegetation indices or land cover classifications, ready for direct use in applications (Figure 1). ARD brings clear benefits, including time savings, consistency, and accessibility that lowers technical barriers for users. However, its standardized processing may not account for regional variations, such as unique atmospheric conditions or terrain complexities, potentially reducing accuracy in specific contexts. Also, reliance on ARD can lead to a lack of transparency in the preprocessing steps, making it difficult for users to assess or modify the corrections applied. Thus, while ARD streamlines workflows, it is not a one-size-fits-all solution and recognizing its limitations is key to leveraging it effectively.\n\n\n\nFigure 1 - From left to right: (1) Sentinel-2 Level-1C TOA reflectance input image, (2) the atmospherically corrected Level-2A surface reflectance image, (3) the output scene classification of the Level-1C product - Source: European Space Agency, link\n\n\n\n\n3.1.1 Dictionary for some new terminologies I learned this week\n\n\n\n\n\n\n\n\n\nTerminology\nMeaning\n\n\n\n\nAnalysis Ready Data\nPre-processed and formatted data that is immediately usable for analysis.\n\n\nAtmospheric attenuation\nThe reduction of signal strength as electromagnetic waves pass through the atmosphere.\n\n\nFeathering\nA technique used to blend the edges of images or data from different sources to create a seamless appearance.\n\n\nImage fusion\nWhen data from multiple sensors/sources is fused together.\n\n\nIrradiance\nDownwelling radiation reaching the Earth from the sun.\n\n\nMosaicking\nThe process of combining multiple satellite images into a single, larger image.\n\n\nNadir\nThe point on the ground directly beneath a satellite or sensor.\n\n\nPath radiance\nRadiance reflected above the surface (before reaching the sensor).\n\n\nPseudo-invariant Features\nAreas on the Earth’s surface that remain relatively unchanged over time.\n\n\nPush broom\nA sensor that moves in a straight line, continuously capturing data across a wide area.\n\n\nRadiance\nAny radiation leaving the Earth.\n\n\nSurface Reflectance\nProportion of light reflected by the Earth’s surface.\n\n\nSpectral radiance\nThe amount of light within a band from a sensor in the field of view (FOV).\n\n\nSolar Azimuth\nThe angle between the sun and true north.\n\n\nSolar Zenith Angle\nThe angle between the local zenith (directly above) and the sun.\n\n\nWhisk broom\nA sensor that scans side to side, capturing data line by line.\n\n\n\n\n\n* Note: Definitions are based on my understanding, lecture notes, and several academic articles.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "W3_Corrections.html#applications",
    "href": "W3_Corrections.html#applications",
    "title": "3  Corrections and Enhancements",
    "section": "3.2 Applications",
    "text": "3.2 Applications\n\nIn this section, I wanted to focus on a specific application and discuss the correction technique used with its evolution over the years. Vegetation monitoring is one of the key applications where correction techniques have evolved significantly over the years, driven by advancements in remote sensing and data processing. Vermote et al. (1997) introduced the 6S (Second Simulation of the Satellite Signal in the Solar Spectrum) model, a radiative transfer code that uses inputs like aerosol optical depth, and water vapor to correct Landsat top-of-atmosphere (TOA) radiance to surface reflectance, enabling accurate NDVI for forest monitoring in regions like the Pacific Northwest. The paper confirms its accuracy through comparisons with other codes, achieving relative errors below 5% in reflectance under standard conditions. Hansen et al. (2008) propose an enhanced method integrating MODIS-derived atmospheric data with Landsat imagery, correcting for haze and clouds to monitor forest cover changes in the Congo Basin. More recently, Basener and Basener (2023) proposed a machine learning approach, training Gaussian Process and deep learning models on 100,000 MODTRAN-simulated spectra to correct hyperspectral imagery. By treating atmosphere atmospheric as noise and learning radiative physics directly, their technique offers adaptability to varied conditions, enhancing NDVI precision.\nWhile 6S offers a robust, standardized baseline, Hansen’s MODIS-Landsat synergy improves large-scale forest monitoring but relies on coarse MODIS inputs, and Basener and Basener’s ML approach excels in flexibility yet demands computational resources. This progression—from physics-based simulation to multi-sensor integration and ML adaptability— highlight how the field is constantly evolving.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "W3_Corrections.html#reflections",
    "href": "W3_Corrections.html#reflections",
    "title": "3  Corrections and Enhancements",
    "section": "3.3 Reflections",
    "text": "3.3 Reflections\n\nReading more on the techniques covered this week made me reflect on how, while the convenience of pre-processed data is undeniable, it’s worth critically questioning whether these standardized corrections and enhancements are universally applicable. For example, atmospheric correction methods designed for one region might not perform as well in another due to differences in environmental conditions. Similarly, relying solely on higher-level products like vegetation indices could overlook nuances in the raw data that might be critical for specific analyses. For instance, Huete et al. (2002) caution that indices like NDVI can be less reliable in arid or semi-arid regions due to soil background effects, emphasizing the need for region-specific adjustments.\nCloud cover is often cited as one of the main challenges in remote sensing studies, but in the region I come from, the Middle East, I don’t think it’s as critical, and I see sandstorms as another key challenge for the region. Sandstorms introduce high levels of particulate matter into the atmosphere, which can distort spectral signals and reduce the accuracy of standard correction methods. This issue is less frequently addressed in mainstream literature. This gap in the literature is something I plan to explore further to better understand how sandstorms impact remote sensing data and how we can develop more robust correction methods for such environments. Moreover, Standardized approaches provide a foundation, but independent thinking and the ability to tailor preprocessing steps to specific project requirements are essential for ensuring the accuracy and reliability of remote sensing analyses.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "W3_Corrections.html#references",
    "href": "W3_Corrections.html#references",
    "title": "3  Corrections and Enhancements",
    "section": "3.4 References",
    "text": "3.4 References\n\nBasener, B., & Basener, A. (2023). Gaussian Process and Deep Learning Atmospheric Correction. Remote Sensing (Basel, Switzerland), 15(3), 649-. https://doi.org/10.3390/rs15030649\nHansen, M. C., Roy, D. P., Lindquist, E., Adusei, B., Justice, C. O., & Altstatt, A. (2008). A method for integrating MODIS and Landsat data for systematic monitoring of forest cover and change in the Congo Basin. Remote Sensing of Environment, 112(5), 2495–2513. https://doi.org/10.1016/j.rse.2007.11.012\nHuete, A., Didan, K., Miura, T., Rodriguez, E. P., Gao, X., & Ferreira, L. G. (2002). Overview of the radiometric and biophysical performance of the MODIS vegetation indices. Remote Sensing of Environment, 83(1), 195–213. https://doi.org/10.1016/S0034-4257(02)00096-2\nVermote, E. F., Tanre, D., Deuze, J. L., Herman, M., & Morcette, J.-J. (1997). Second Simulation of the Satellite Signal in the Solar Spectrum, 6S: an overview. IEEE Transactions on Geoscience and Remote Sensing, 35(3), 675–686. https://doi.org/10.1109/36.581987",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html",
    "href": "W4_Policy.html",
    "title": "4  EO Data in Policy Making",
    "section": "",
    "text": "4.1 Green Riyadh Initiative\nThis week, we explored how Earth Observation (EO) data can support policymaking. Below is a usecase illustrating how EO data can address a metropolitan policy challenge in Riyadh.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EO Data in Policy Making</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#green-riyadh-initiative",
    "href": "W4_Policy.html#green-riyadh-initiative",
    "title": "4  EO Data in Policy Making",
    "section": "",
    "text": "The Green Riyadh Initiative is a cornerstone of Saudi Arabia’s Vision 2030, aiming to transform Riyadh into a greener, more sustainable city. The project plans to plant 7.5 million trees across the city, increasing green cover from 1.5% to 9.1% by 2030. It seeks to combat urban heat, improve air quality, and enhance biodiversity, creating a healthier, more sustainable city.\nGrowing up in Riyadh, I’ve witnessed firsthand how the city’s extreme heat and urban sprawl have intensified over time. During peak summer, it’s not uncommon to see 45°C or higher, and the urban heat island (UHI) effect makes certain areas feel even hotter. Studies have shown that the UHI effect can increase temperatures by 4-6°C in densely built areas, and at its peak, it may exceed 10°C (Santamouris & Vasilakopoulou, 2023), exacerbating the city’s heat challenges.\nAir quality is another pressing issue, with high levels of particulate matter (PM2.5) and other pollutants, largely due to Riyadh’s car-centric urban design. Urban greenery has been proven to mitigate these challenges by providing shade, reducing car dependency, and absorbing pollutants (Gössling, 2020). By creating shaded walkways and green corridors, the initiative encourages walking and cycling, offering a sustainable alternative to short car trips.\nSince its launch in 2019 by the Royal Commission for Riyadh City (RCRC), the initiative has made significant progress. New parks and green corridors are emerging in neighborhoods, and trees now line almost all major highways. The RCRC has designed a master plan identifying key areas for afforestation, including:\n\n3,330 neighborhood gardens\n2,000 car parking sites\n16,400 Kilometers of streets and roads\n272 Kilometers of valleys\n175,000 Square Kilometers of empty land\n\nRCRC has also established clear (guidelines) for selecting plant species suitable for Riyadh’s climate and urban environment, prioritizing drought-resistant and native species to ensure sustainability.\nThe initiative aligns with national goals under Vision 2030, including the Saudi Green Initiative, which aims to plant 10 billion trees nationwide, guided by three overarching targets: emissions reduction, afforestation and land regeneration, and land and sea protection. By increasing green spaces, Riyadh contributes directly to these targets, promoting environmental sustainability and improving quality of life. It also connects to global goals, such as the United Nations Sustainable Development Goals (SDGs), particularly SDG 11 (Sustainable Cities and Communities), SDG 13 (Climate Action), and SDG 15 (Life on Land).\n\n\nBelow is a video that gives an overview about the the initiative:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EO Data in Policy Making</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#use-of-eo-data",
    "href": "W4_Policy.html#use-of-eo-data",
    "title": "4  EO Data in Policy Making",
    "section": "4.2 Use of EO Data",
    "text": "4.2 Use of EO Data\n\nIn this section, I’ll focus on how remotely sensed data can support a key objective of the Green Riyadh Initiative: reducing urban heat. While the initiative’s published documents do not explicitly mention the use of EO data, the reported progress suggests that change detection and monitoring are likely being employed. The initiative identifies specific areas and priorities for afforestation, with targeted numbers for different categories such as neighborhood gardens, parking sites, and valleys.\nThermal imagery from satellites like Landsat 8 and Sentinel-3 is one key application to evaluate land surface temperature (LST) reductions after tree planting (Figure 1), as demonstrated in studies like Sadek et al. (2020) and Xu et al. (2023). However, incorporating LiDAR and high-resolution multispectral data (e.g., from Sentinel-2) can take this a step further by enabling precise microclimate mapping. LiDAR’s ability to map canopy height (link) and density at resolutions down to 1 meter, paired with Sentinel-2’s 10-meter multispectral bands could model localized climate conditions at a granular level.  By processing Sentinel-2 data to calculate vegetation indices like the Normalized Difference Vegetation Index (NDVI) and Enhanced Vegetation Index (EVI), planners can monitor the health, photosynthesis, and transpiration of Riyadh’s urban greenery. These indices, when overlaid with canopy maps, could generate microclimate maps allowing for targeted afforestation efforts where tree planting can have the most significant cooling impact.\n\n\n\nFigure 1 - Seasonal and spatial variability of LST in Riyadh - Source: Alghamdi et al. 2021\n\n\nTo enrich this microclimate mapping, datasets from the Copernicus Climate Change Service (C3S) offer a broader climatic lens. The C3S ERA5-Land dataset (link), delivering hourly and monthly variables such as 2-meter air temperature, soil moisture, and evapotranspiration at 9-km resolution, can be downscaled and fused with Sentinel-2 NDVI to reveal, for instance, how afforestation along Riyadh’s wadis lowers near-surface air temperature by 1-2°C while boosting humidity in dry seasons, tailoring planting strategies to maximize cooling. Complementing this, the C3S Seasonal Forecast dataset (link) (e.g., ECMWF’s SEAS5 model) provides six-month predictions of temperature and precipitation anomalies, allowing policymakers to optimize planting schedules—perhaps prioritizing early spring in dust-prone areas like Diriyah to enhance tree establishment and microclimatic benefits.\nRelating this to the discussed study during lecture by MacLachlan et al. 2021, which emphasized the importance of strategically placing trees to optimize temperature reduction, microclimate mapping emerges as a vital tool for enhancing the effectiveness of the Green Riyadh Initiative. The initiative’s published documents, while rich with targets, lack the fine-scale spatial detail needed to prioritize plantings where heat mitigation is most urgent. Microclimate mapping, powered by EO data, fills this gap by identifying localized heat sinks offering a pathway to maximize ecological returns on investment. Without such granularity, the initiative risks diluting its cooling potential across less impactful sites, underscoring the transformative role EO-driven mapping could play in sharpening policy focus and execution.\n\n\n\nIllustration of the Green Riyadh Initiative’s greening efforts along a highway at the city’s outer boundary - Source: Green Riyadh, 2023",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EO Data in Policy Making</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#reflections",
    "href": "W4_Policy.html#reflections",
    "title": "4  EO Data in Policy Making",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\n\nWith the big transformation Riyadh is seeing these years, it was challenging to identify a single policy or a challenge that EO can support, but the Green Riyadh Initiative stands out for its citywide reach. Shaded walkways, new parks, and increased pedestrian activity are visible changes, but whether these efforts meaningfully reduce LST remains unclear without EO data. This highlights a critical gap: without leveraging remote sensing technologies, we cannot fully understand or measure the success of such initiatives. While researching I came across an intriguing finding from an urban scale study on one of Riyadh’s neighborhoods by Haddad et al. (2024). They highlighted how non-irrigated plants help cool cities at night but aren’t very effective during the day, whereas Irrigated trees are the best option for cooling cities both day and night. This aligns with micro-scale studies, such as Zölch et al. (2016), which emphasize the importance of detailed EO data in understanding localized climate impacts and optimizing urban greening strategies.\nOn a broader note, although not related to my use case, one of the key examples that highlight EO data’s power comes from the International Energy Agency (IEA). In 2022, they reported that satellite imagery revealed methane emissions from the energy sector to be 70% higher than official government reports. This underscores EO’s potential to provide accurate, actionable insights that can drive policy decisions. In conclusion, incorporating EO data into more policies isn’t just an enhancement—it’s essential for grounding decisions in evidence, ensuring initiatives like Green Riyadh deliver measurable benefits, and tackling global challenges with precision and accountability.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EO Data in Policy Making</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#references",
    "href": "W4_Policy.html#references",
    "title": "4  EO Data in Policy Making",
    "section": "4.4 References",
    "text": "4.4 References\n\nAlghamdi, A. S., Alzhrani, A. I., & Alanazi, H. H. (2021). Local Climate Zones and Thermal Characteristics in Riyadh City, Saudi Arabia. Remote Sensing, 13(22), 4526. https://doi.org/10.3390/rs13224526\nGössling, S. (2020). Why cities need to take road space from cars - and how this could be done. Journal of Urban Design, 25(4), 443–448. https://doi.org/10.1080/13574809.2020.1727318\nHaddad, S., Zhang, W., Paolini, R., Gao, K., Altheeb, M., Al Mogirah, A., Bin Moammar, A., Hong, T., Khan, A., Cartalis, C., Polydoros, A., & Santamouris, M. (2024). Quantifying the energy impact of heat mitigation technologies at the urban scale. Nature Cities, 1(1), 62–72. https://doi.org/10.1038/s44284-023-00005-5\nInternational Energy Agency. (2022, February 23). Methane emissions from the energy sector are 70% higher than official figures. https://www.iea.org/news/methane-emissions-from-the-energy-sector-are-70-higher-than-official-figures\nMacLachlan, A., Biggs, E., Roberts, G., & Boruff, B. (2021). Sustainable City Planning: A Data-Driven Approach for Mitigating Urban Heat.\nSadek, M., Beshr, A. A., Kaloop, M. R., Liu, G., Co, Y., Mustafa, E. K., Zarzoura, F., & Zhao, D. (2020). Study for Predicting Land Surface Temperature (LST) Using Landsat Data: A Comparison of Four Algorithms. Advances in Civil Engineering, 2020(2020), 1–16. https://doi.org/10.1155/2020/7363546\nSantamouris, M., & Vasilakopoulou, K. (2023). Recent progress on urban heat mitigation technologies. Science Talks (Online), 5, 100105-. https://doi.org/10.1016/j.sctalk.2022.100105\nXu, X., Pei, H., Wang, C., Xu, Q., Xie, H., Jin, Y., Feng, Y., Tong, X., & Xiao, C. (2023). Long-term analysis of the urban heat island effect using multisource Landsat images considering inter-class differences in land surface temperature products. The Science of the Total Environment, 858, 159777-. https://doi.org/10.1016/j.scitotenv.2022.159777\nZölch, T., Maderspacher, J., Wamsler, C., & Pauleit, S. (2016). Using green infrastructure for urban climate-proofing: An evaluation of heat mitigation measures at the micro-scale. Urban Forestry & Urban Greening, 20, 305–316. https://doi.org/10.1016/j.ufug.2016.09.011",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EO Data in Policy Making</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html",
    "href": "W6_GEE.html",
    "title": "5  Google Earth Engine (GEE)",
    "section": "",
    "text": "5.1 Introduction to GEE",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#introduction-to-gee",
    "href": "W6_GEE.html#introduction-to-gee",
    "title": "5  Google Earth Engine (GEE)",
    "section": "",
    "text": "This week we were introduced to Google Earth Engine (GEE), the one-hour lecture was heavy, full of new terminologies and concept. The session covered: code syntax, objects, and functions; the differences between client-side and server-side processes; and key technical ideas like scaling, projections, and reductions. We also explored common analytical techniques, including zonal statistics, regression, and principal component analysis. \nHere, the focus will be on two key components that i think are essential building blocks for anyone beginning to work with Google Earth Engine (GEE): scaling and reduction\nIn GEE, the concept of scale refers to the resolution of the data—how large or small each pixel is, such as 10 meters, 30 meters, or 500 meters. Unlike traditional GIS tools, where the resolution is determined by the input data, GEE takes a distinctive approach by tying scale to the output. This process occurs seamlessly on the server side, enabling users to handle datasets with varying native resolutions without manual resizing, reprojection, or alignment. If the scale isn’t explicitly set, GEE assigns one based on the context—typically the native resolution of the input data or the zoom level of the map in view. This flexibility stands out as one of GEE’s greatest strengths, removing the time-consuming preprocessing steps common in traditional workflows. Yet, this convenience brings a layer of responsibility: users must thoughtfully choose a scale that fits their analysis needs. A scale too coarse risks hiding critical patterns or details, while one too fine can make the analysis overly complex or computationally intensive.\nReduction is another fundamental concept that plays a crucial role in analyzing and summarizing geospatial data. It refers to aggregating pixel values into concise statistics, such as means, sums, or extremes, enabling analysis across spatial or temporal dimensions. For example, reducing a time series of satellite images to a single composite can reveal trends in vegetation health (e.g., NDVI) or land surface temperature over time (Zhang et al., 2003). Reduction techniques in GEE can be broadly categorized into:\n\nTemporal Reduction (imageCollection.reduce()): Aggregating data over time, such as calculating seasonal or annual averages. This is particularly useful for monitoring long-term environmental changes .\nSpatial Reduction:\n• reduceRegion(), reduceRegions() - (Figure 1): Summarizing data over a geographic area to compute statistics for polygons or points.\n• reduceNeighborhood() - (Figure 2): Aggregating data within a kernel or moving window, which is useful for smoothing data or detecting spatial patterns (e.g., edge detection or texture analysis).\nSpectral Reduction (image.reduce()): Combining or summarizing data across multiple bands, such as computing vegetation indices or band-specific statistics.\n\n\n\n\n\nFigure 1 - Illustration of a Reducer applied to an ImageCollection.  Source\n\n\n\n\n\nFigure 2 - Illustration of reduceNeighborhood(), where the reducer is applied in a kernel  Source\n\n\n\nScale and reduction are foundational to geospatial analysis in GEE: scale defines the resolution, shaping the framework for analysis, while reduction extracts meaningful insights from data. Mastering these concepts goes beyond technical skill—it’s about balancing detail and simplicity to address real-world questions effectively. I came across a very interesting metaphor in a post (unfortunately, I’ve lost the link, but it’s too good to skip): it’s almost like cooking— scale preps the ingredients, chopping them just right, and reduction simmers them into a perfect sauce. Get one wrong, and the whole dish is off. In GEE, this balance feels like an art form, one that takes practice to master but opens up a world of possibilities once you do. So it’s not just about the tools; it’s about understanding the process, experimenting, and refining your approach.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#applications",
    "href": "W6_GEE.html#applications",
    "title": "5  Google Earth Engine (GEE)",
    "section": "5.2 Applications",
    "text": "5.2 Applications\n\nGEE has emerged as a critical tool for accessing and analyzing geospatial data across various disciplines. Pham-Duc et al. (2023) reviewed trends in GEE usage by analyzing peer-reviewed articles up to 2023, revealing that nearly 50% of applications are concentrated in the fields of Earth and planetary sciences and environmental science (Figure 3). This dominance aligns with these fields’ reliance on processes that are inherently spatial, temporal, and data-intensive. GEE, with its extensive satellite imagery archive, cloud-based processing power, and real-time capabilities, is ideally suited for studying phenomena such as deforestation, climate change, and ecosystem dynamics.\n\n\n\nFigure 3 - Disciplines where GEE has been applied\n\n\nGlobal Forest Cover Change by Hansen et al. (2013) stands out as one of the pioneering applications in quantifying global forest extent and change in a high-resolution (30-meter) thematic map. This work has become a cornerstone for researchers studying deforestation and forest dynamics at a global scale (Heino et al., 2015; Tyukavina et al., 2017). In addition, GEE has played a critical role in real-time disaster monitoring, such as during the recent California wildfires, where it enabled rapid mapping of fire perimeters, burn severity, and smoke dispersion using satellite data (Anderson, 2025)\nGEE’s potential extends beyond environmental and disaster applications. By providing access to high-performance computing and vast datasets, it empowers researchers and policymakers to address global challenges with remarkable precision and the ability to scale up solutions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#reflections",
    "href": "W6_GEE.html#reflections",
    "title": "5  Google Earth Engine (GEE)",
    "section": "5.3 Reflections",
    "text": "5.3 Reflections\n\nThis week, one main thought that continually came to my mind while learning more about GEE is why it hasn’t been more widely adopted by governments and large companies, especially given its real-time capabilities and open-access model. While GEE is still popular among researchers and environmental scientists for its ability to process vast amounts of satellite data quickly, its reliance on coding (JavaScript or Python) can be a significant barrier for non-technical users. In contrast, Esri’s ArcGIS dominates in global policy and spatially based applications, offering a comprehensive, enterprise-grade GIS suite with user-friendly interfaces like ArcGIS Pro and ArcGIS Online. These features make Esri more accessible for policymakers and organizations that may lack technical expertise but require robust tools for decision-making. However, Esri’s dominance raises concerns about accessibility for underfunded regions and institutions that cannot afford proprietary software licenses. GEE’s open-access model has the potential to democratize geospatial analysis, particularly in developing countries or smaller organizations, but its steep learning curve and lack of intuitive graphical interfaces limit its broader adoption. Additionally, GEE’s cloud-based nature, while powerful, can be a limitation for users handling sensitive data that requires offline processing. That said, despite GEE’s established collaborations with the United Nations and NGOs—such as UNEP and FAO for environmental monitoring and disaster response—I think its niche as a coding-heavy, research-focused tool limits its appeal for widespread governmental and corporate use compared to Esri’s enterprise-ready solutions. GEE could expand its influence by enhancing user-friendly interfaces, integrating AI automation, and forming deeper enterprise partnerships, potentially complementing Esri’s strengths to create a more inclusive geospatial ecosystem for global policy and decision-making.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#references",
    "href": "W6_GEE.html#references",
    "title": "5  Google Earth Engine (GEE)",
    "section": "5.4 References",
    "text": "5.4 References\n\nGoogle Earth Engine Team. (n.d.). Introduction to reducers. Google Developers. https://developers.google.com/earth-engine/guides/reducers_intro\nHansen, M., Potapov, P., Moore, R., Hancher, M. (2013). The first detailed maps of global forest change. Google Research Blog. Retrieved from https://research.google/blog/the-first-detailed-maps-of-global-forest-change/\nHeino, M., Kummu, M., Makkonen, M., Mulligan, M., Verburg, P. H., Jalava, M., & Räsänen, T. A. (2015). Forest loss in protected areas and intact forest landscapes: A global analysis. PLOS ONE, 10(10), e0138918. https://doi.org/10.1371/journal.pone.0138918\nJohn Anderson (2025). Supporting the Los Angeles Community During the Wildfires. Google Earth. Retrieved from https://medium.com/google-earth/supporting-the-los-angeles-community-during-the-wildfires-f67614046794\nPham-Duc, B., Nguyen, H., Phan, H., & Tran-Anh, Q. (2023). Trends and applications of Google Earth Engine in remote sensing and earth science research; a bibliometric analysis using SCOPUS database. Earth Science Informatics, 16(3), 2355–2371. https://doi.org/10.1007/s12145-023-01035-2\nTyukavina, A., Hansen, M. C., Potapov, P. V., Stehman, S. V., Smith-Rodriguez, K., Okpa, C., & Aguilar, R. (2017). Types and rates of forest disturbance in Brazilian Legal Amazon, 2000–2013. Science Advances, 3(4), e1601047–e1601047. https://doi.org/10.1126/sciadv.1601047\nZhang, X., Friedl, M. A., Schaaf, C. B., Strahler, A. H., Hodges, J. C. F., Gao, F., Reed, B. C., & Huete, A. (2003). Monitoring vegetation phenology using MODIS. Remote Sensing of Environment, 84(3), 471–475. https://doi.org/10.1016/S0034-4257(02)00135-9",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA00023 Learning Diary",
    "section": "",
    "text": "Overview\nWelcome to my learning diary, where I will reflect on my weekly engagement with course content, readings, and practical materials using Quarto in R. Each book will highlight key insights and applications of the concepts discussed, capturing what I have learned and found interesting each week. The structure of my diary includes a summary of the weekly material, an exploration of how these concepts have been applied in literature or policy, and personal reflections that contextualize the skills and knowledge gained, considering their relevance and potential future applications based on my interests.\n\ndon’t forget to add a header\nUpdate Xaringan slides link # About me\nUpdate reflection on Xaringan\nmake sure to critical engage with each section, shall i highlight?\n\nHello, this is Norah",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "W7_classificarion_1.html",
    "href": "W7_classificarion_1.html",
    "title": "6  Classification I",
    "section": "",
    "text": "6.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification I</span>"
    ]
  },
  {
    "objectID": "W7_classificarion_1.html#summary",
    "href": "W7_classificarion_1.html#summary",
    "title": "6  Classification I",
    "section": "",
    "text": "This week, we explored a foundational concept in remote sensing: classification, the process of categorizing image pixels into distinct classes based on their spectral characteristics. The first part of the lecture covered real-world applications like tracking urban sprawl, linking air pollution to land use, and pinpointing deforestation hotspots. The forest monitoring and illegal logging case was really interesting. In that case, Landsat imagery was pre-processed—resampled, converted into top-of-atmosphere reflectance, and cleared of cloud cover—before introducing a key concept: “creating metrics”. This essential step, performed before classification, involves deriving specific numerical measures or indices (e.g., maximum/minimum reflectance, mean values, or temporal slopes) from spectral bands across a time series. These metrics transform raw data into a “feature space,” allowing algorithms to identify patterns, such as the detection of 5,000 deforestation sites in Brazil (Hansen et al., 2013).\nThe second part shifted focus to how classification is done, covering general machine learning algorithms (e.g., linear regression, random forest) and how they are applied in a spatial context. Linear regression is primarily utilized to estimate continuous measures, such as the distribution of atmospheric pollutants across geographic expanses, providing a foundational approach to predicting trends over space and time. Random Forests, conversely, assemble multiple decision trees that analyze random data samples to classify land types—forests, urban areas, or fields—enhancing accuracy through collective evaluation. This method leverages bootstrapping and random variable selection to enhance reliability, as demonstrated in its application to land cover mapping.\nThe lecture referenced Maximum Likelihood, an older probability-based method, but we did not cover it, as it is not used anymore. Looking at why, I learned how its reliance on assumptions of uniform data distribution is considered less effective for handling the diverse and complex imagery prevalent today, particularly given advancements in computational technology (Richards, 2006).\nDespite these advances, challenges and limitations persist. Decision trees risk overfitting, where models excessively conform to training data, reducing their ability to generalize. Mitigation through pruning or setting minimum leaf sizes addresses this, though it increases complexity and requires precise tuning. Additionally, the lecture highlights a trade-off between complexity and interpretability: advanced classifiers like Random Forests and Support Vector Machines offer high accuracy but can might become “black boxes,” reducing interpretability.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification I</span>"
    ]
  },
  {
    "objectID": "W7_classificarion_1.html#applications",
    "href": "W7_classificarion_1.html#applications",
    "title": "6  Classification I",
    "section": "6.2 Applications",
    "text": "6.2 Applications\n\nWhile the summary emphasized algorithmic approaches (e.g., Random Forests (RF), Support Vector Machines (SVM)), another critical distinction in classification methods we covered this week lies in supervised and unsupervised techniques, each offering unique applications. Unsupervised approaches, like k-means clustering, identify natural groupings without prior labels, offering flexibility for exploratory tasks where the goal is discovery rather than prediction. On the other hand, Supervised classification, where algorithms like Random Forests or SVM are trained on labeled datasets, excels in precise, goal-driven tasks. Both algorithms, are supervised learning methods, but they differ significantly in their approach, data handling, and performance characteristics. Here I’ll focus on highlighting that:\n- SVM: \nSimilar to logistic regression, SVM minimizes generalization error without distribution assumptions, separating classes (e.g., forest vs. farmland) through an optimal hyperplane. It excels in high-dimensional applications, one example is detecting oil spills in synthetic aperture radar (SAR) imagery for environmental monitoring. Matkan et al. (2013) applied SVM to SAR images from the Gulf of Mexico, achieving high accuracy by training on texture and intensity features to differentiate oil-contaminated water from clean surfaces. Optimal performance required tuning parameters C (regularization) and gamma (kernel scale) through grid search and cross-validation, ensuring a balance between margin maximization and classification precision. However, this tuning process is computationally demanding, particularly for large datasets, and SVM’s limited interpretability—offering no direct insight into feature contributions—may restrict its use in applications requiring transparent decision-making, such as policy reporting for spill response strategies.\n- RF: \nThis methods ability to quantify feature importance makes it valuable for various applications. One example is its role in analyzing burn severity drivers, as demonstrated by Huang et al. (2020) in California’s coastal mountains. Their study used RF to model burn severity, identifying slope, aspect, fuel moisture, and long-term climate as key predictors. During droughts, RF revealed that low fuel moisture and high climatic water deficit doubled high-severity burn areas. However, while efficient and interpretable, this approach may oversimplify localized fire behavior, particularly in heterogeneous landscapes where small differences in terrain or fuel types affect fire intensity. Additionally, correlated features (e.g., temperature and drought indices) can distort importance rankings, requiring careful variable selection.\n\n\n\nFigure 1 - RF Vs SVM approach.  Source",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification I</span>"
    ]
  },
  {
    "objectID": "W7_classificarion_1.html#reflections",
    "href": "W7_classificarion_1.html#reflections",
    "title": "6  Classification I",
    "section": "6.3 Reflections",
    "text": "6.3 Reflections\nReflecting on this lecture, I found it easy to follow and connect with, probably because classification appears in many familiar research areas like city planning or detecting change. Before this, I had some knowledge of machine learning, but not within remote sensing.\nA few years ago, in 2020, I attempted to detect building footprints in Riyadh using an Esri machine learning tool. The results were inaccurate, and at the time, I assumed buildings shared similar traits like materials across regions. I didn’t fully understand how Riyadh’s buildings differ in spectral and structural characteristics from those in the tool’s training data and the big impact this has. Now, I see how methods like SVM classification, introduced in the lecture, could improve such work by adjusting to local details.\nThe lecture also raised a broader consideration: the tendency to utilize the most accurate tools without considering their suitability for a given problem. The lecture showed Maximum Likelihood isn’t used much anymore because it can’t keep up with today’s messy data (Richards, 2006), which makes me think: how often do we pick flashy tech over what actually works for the problem?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification I</span>"
    ]
  },
  {
    "objectID": "W7_classificarion_1.html#references",
    "href": "W7_classificarion_1.html#references",
    "title": "6  Classification I",
    "section": "6.4 References",
    "text": "6.4 References\n\nHansen, M.C., Potapov, P.V., Moore, R., Hancher, M., Turubanova, S.A., Tyukavina, A., Thau, D., Stehman, S.V., Goetz, S.J., Loveland, T.R., Kommareddy, A., Egorov, A., Chini, L., Justice, C.O., Townshend, J.R.G., 2013. High-Resolution Global Maps of 21st-Century Forest Cover Change. Science 342, 850–853.\nHuang, Y., Jin, Y., Schwartz, M. W., & Thorne, J. H. (2020). Intensified burn severity in California’s northern coastal mountains by drier climatic condition. Environmental Research Letters, 15(10), 104033-. https://doi.org/10.1088/1748-9326/aba6af\nMatkan, A. A., Hajeb, M., & Azarakhsh, Z. (2013). Oil spill detection from SAR image using SVM based classification. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences., XL-1/W3, 55–60. https://doi.org/10.5194/isprsarchives-XL-1-W3-55-2013\nRichards, J. A. (John A., & Jia, X. (2006). Remote sensing digital image analysis : an introduction. (4th ed. / John A. Richards, Xiuping Jia). Springer.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification I</span>"
    ]
  },
  {
    "objectID": "W8_classification_2.html",
    "href": "W8_classification_2.html",
    "title": "7  Classification II",
    "section": "",
    "text": "7.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "W8_classification_2.html#applications",
    "href": "W8_classification_2.html#applications",
    "title": "7  Classification II",
    "section": "7.2 Applications",
    "text": "7.2 Applications\n\n1. Pixel-Based Classification: Limitations in Urban Landscapes\nPixel-based methods classify individual pixels based solely on their spectral signatures. While effective for large-scale studies like the global deforestation mapping by Hansen et al. (2013), which was discussed in the lecture, they face challenges in urban environments, including:\n\n\nSpectral ambiguity between different urban materials\nPrevalence of mixed pixels in high-resolution imagery\nInability to incorporate spatial relationships between features\n\n\nThese limitations highlight a deeper issue: pixel-based methods assume spectral data alone can tell the full story, ignoring the spatial relationships that define urban complexity. This raises doubts about their suitability for tasks high level of details, where material diversity and small-scale layouts challenge spectral clarity. Could over-reliance on this method lead to oversimplified conclusions in rapidly changing cities?\n2. Object-Based Classification: A Spatial-Spectral Solution\nObject-based image analysis (OBIA) addresses these limitations through a hierarchical approach that:\n\n\nSegments imagery into meaningful objects based on both spectral and spatial characteristics\nIncorporates contextual information through shape, texture, and relational metrics\nUtilizes multi-scale analysis to capture features at appropriate levels of detail\n\n\nThe efficacy of this approach was demonstrated by Myint et al. (2011), whose comparative analysis of land cover in Phoenix showed a significant accuracy improvement—90.4% for object-based classification versus 67.6% for pixel-based classification, as visually evidenced in their Figure 1 below. This finding underscores the effectiveness of incorporating spatial context in classification efforts. However, OBIA’s strength—its reliance on segmentation—also introduces vulnerabilities. Selecting the appropriate scale is subjective and risks inconsistency if poorly chosen. This subjectivity could undermine reliability in large-scale projects, where uniform standards are essential. Moreover, the added computational demand of OBIA prompt the question: does its accuracy justify the resource cost, particularly in settings with limited resources where simpler methods might be sufficient?\n\n\n\nFigure 1 - (a) Test image; (b) output map produced by the object-based approach; (c) output map produced by the classical per-pixel classifier. Note: Cyan = buildings; orange = unmanaged soil; light green = grass; gray = other impervious surfaces, purple = swimming pools; dark green = trees and shrubs; and blue = lakes and ponds. - Source: (Myint et al., 2011)\n\n\n3. Current Trends and Future Directions\nRecent studies suggest that hybrid approaches, such as using OBIA for segmentation followed by Random Forest classification, can enhance classification outcomes (Ma et al., 2017). This integration of methodologies represents a promising direction, combining the strengths of both pixel-based and object-based approaches to improve accuracy and reliability.\nAdditionally, emerging trends, including deep learning Convolutional Neural Networks (CNNs), further complicate these paradigms by automating spatial feature extraction. This advancement enables pixel-object fusion for applications such as slum mapping and wetland delineation (Zhang et al., 2019). The potential of these technologies to revolutionize remote sensing analysis is significant, suggesting a future where classification methods are more adaptive and sophisticated.\nThis methodological evolution has established object-based approaches as the standard for urban remote sensing applications. However, the choice between pixel-based and object-based methods ultimately depends on specific research objectives, the scale of analysis, and available computational resources.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "W8_classification_2.html#summary",
    "href": "W8_classification_2.html#summary",
    "title": "7  Classification II",
    "section": "",
    "text": "This week’s lecture is a continuation of classification related concepts, it was condensed, but I’ll try to briefly summarize main ideas grouped into the following sections:\n\n\n7.1.1 Advanced Analytical Techniques\n\nRelated content is explained in detail in the upcoming application section, where it makes sense to present it alongside practical examples. I opted not to explain it here to avoid redundancy and maintain conciseness.\n\n\n\n7.1.2 Evaluation and Performance Metrics\n\nIn machine learning, accuracy assessment is essential for validating model outputs. For remote sensing applications, this typically focuses on Producer Accuracy (PA) and User Accuracy (UA), as shown in Figure 1. These metrics are complemented by Overall Accuracy (OA), which aggregates correctly classified pixels across all categories. The F1 score provides a balanced measure between PA and UA, ranging from 0 (poor performance) to 1 (perfect balance). The Receiver Operating Characteristic curve (ROC) offers another perspective by plotting true versus false positive rates, with the Area Under the Curve (AUC) quantifying performance from 0.5 (random) to 1 (optimal).\nWhile these metrics work well with balanced data, the lecture highlighted how imbalances are common. Additionally, the equal weighting of PA and UA in the F1 score may not suit all applications, like urban mapping, where false negatives—missing urban pixels—matter more than false positives. For ROC in the other hand, the binary focus may oversimplify multi-class classifications.\n\n\n\nFigure 1 - Producer and User Accuracy (Source: Banko et al., 1998)\n\n\n\n\n\n7.1.3 Testing Data Selection and Spatial Validation\n\nTo reinforce the accuracy metrics outlined in the previous section, different validation strategies can be used. The most straightforward approach—a simple train-test split—reserves a subset of the data for final evaluation. More rigorous methods, like k-fold cross-validation, cycle through multiple data partitions, training the model on all but one-fold and testing on the remaining one.\nBut spatial data has a unique issue: spatial autocorrelation, where nearby locations tend to be similar—which means traditional validation can dramatically overestimate performance (Tobler, 1970). Spatial cross-validation methods address this by enforcing geographic separation between training and test sets. However, lack of clear rules for how much spatial separation is enough introduces uncertainty. Current best practices recommend spatially blocked designs that maintain realistic independence while remaining computationally feasible (Roberts et al., 2017), though the choice ultimately depends on each project’s specific spatial structure and resource constraints\n\n“Everything is related to everything else, but near things are more related than distant things”\n— Tobler, 1970",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "W8_classification_2.html#reflections",
    "href": "W8_classification_2.html#reflections",
    "title": "7  Classification II",
    "section": "7.3 Reflections",
    "text": "7.3 Reflections\nThe tension between methodological rigor and practical implementation proves especially significant in emerging fields like climate resilience planning. While spatially blocked cross-validation effectively addresses spatial autocorrelation (Roberts et al., 2017), its static geographic partitions may not adequately capture dynamic systems where spatial relationships evolve over time.\nThis limitation highlights a broader conceptual shift in geospatial analysis—from purely statistical validation toward impact-aware evaluation frameworks. The prioritization of specific error types (e.g., false negatives in urban mapping) extends beyond technical considerations to reflect fundamental value judgments about risk assessment. For example: • Environmental justice applications may require differential weighting of errors, where underestimating pollution exposure (false negatives) carries greater consequences than misclassifying green spaces • Disaster response systems could employ adaptive significance metrics, with error costs scaling dynamically based on event likelihood\nThese observations resonate particularly with my planned dissertation work on detection methodologies. The choice between pixel-based and object-based approaches—a fundamental analytical decision—directly influences detectable patterns and outcomes. This realization underscores the need for deeper investigation into how methodological selections shape research findings across different applications",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "W8_classification_2.html#references",
    "href": "W8_classification_2.html#references",
    "title": "7  Classification II",
    "section": "7.4 References",
    "text": "7.4 References\n\nBanko, Gebhard. (1998). A Review of Assessing the Accuracy of Classifications of Remotely Sensed Data and of Methods Including Remote Sensing Data in Forest Inventory.\nBrenning, A. (2012). Spatial cross-validation and bootstrap for the assessment of prediction rules in remote sensing: The R package sperrorest. 2012 IEEE International Geoscience and Remote Sensing Symposium, 5372–5375. https://doi.org/10.1109/IGARSS.2012.6352393\nMa, L., Li, M., Ma, X., Cheng, L., Du, P., & Liu, Y. (2017). A review of supervised object-based land-cover image classification. ISPRS Journal of Photogrammetry and Remote Sensing, 130, 277–293. https://doi.org/10.1016/j.isprsjprs.2017.06.001\nMyint, S. W., Gober, P., Brazel, A., Grossman-Clarke, S., & Weng, Q. (2011). Per-pixel vs. object-based classification of urban land cover extraction using high spatial resolution imagery. Remote Sensing of Environment, 115(5), 1145–1161. https://doi.org/10.1016/j.rse.2010.12.017\nRoberts, D. R., Bahn, V., Ciuti, S., Boyce, M. S., Elith, J., Guillera‐Arroita, G., Hauenstein, S., Lahoz‐Monfort, J. J., Schröder, B., Thuiller, W., Warton, D. I., Wintle, B. A., Hartig, F., & Dormann, C. F. (2017). Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure. Ecography (Copenhagen), 40(8), 913–929. https://doi.org/10.1111/ecog.02881\nTobler, W. R. (1970). A Computer Movie Simulating Urban Growth in the Detroit Region. Economic Geography, 46, 234–240. https://doi.org/10.2307/143141\nZhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., & Atkinson, P. M. (2019). Joint Deep Learning for land cover and land use classification. Remote Sensing of Environment, 221, 173–187. https://doi.org/10.1016/j.rse.2018.11.014",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html",
    "href": "W9_SAR.html",
    "title": "8  SAR",
    "section": "",
    "text": "8.1 Summary\nFigure 2 - SAR Backscatter Mechanisms - Source: [EUSI]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>SAR</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#summary",
    "href": "W9_SAR.html#summary",
    "title": "8  SAR",
    "section": "",
    "text": "In the introduction page of this diary, I briefly discussed the distinction between active and passive sensors. However, much of this module has focused on passive optical sensors. This week, we concluded the module by shifting our attention to Synthetic Aperture Radar (SAR) - an active technology with unique advantages: day/night operation (independent of solar illumination), all-weather capability, subsurface penetration potential, and flexible imaging configurations ranging from wide-area coverage to high-resolution mapping.\n\n\n8.1.1 How does SAR Work?\nSAR emits radiofrequency pulses toward Earth’s surface from a moving platform, like a satellite. These radar waves bounce back after interacting with features below, carrying information about their size, orientation, composition, and texture. In radar systems, resolution improves with a larger antenna aperture, much like how a bigger aperture in optical cameras boosts light collection for sharper images. Yet, for a satellite to achieve high-resolution Earth imaging, a physical antenna would need to be impractically massive—spanning hundreds of meters. SAR overcomes this limitation by “synthesizing” a large aperture using motion. As the satellite travels along its orbit, it transmits multiple radar pulses and records their reflections (Figure 1) . By combining these signals over time, SAR simulates the effect of a massive antenna, achieving high resolution without the need for an impractical physical structure. While groundbreaking, this approach has inherent limitations. The technique’s dependence on precise motion makes it sensitive to platform stability - even minor deviations in the satellite’s trajectory can compromise the synthesized aperture, potentially degrading image quality. Additionally, the complex signal processing demands substantial computational resources and introduces geometric distortions that require correction.\n\n\n\nFigure 1 - Depiction of synthetically increasing aperture for a given sensor and using concepts from camera optics to analogize the processes. - Source: Esri\n\n\n\n\nThe reflections SAR captures, known as “backscatter,” vary by scattering mechanism, revealing insights into the surface type of an object (Figure 2). However, surface properties are complicated, Campbell (2002) highlights how these complexities create an “inverse problem” where multiple surface conditions may generate identical backscatter responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBackscatter from a rough surface (single bounce)\n\n\n\n\n\n\n\nBackscatter from a smooth surface (single bounce)\n\n\n\n\n\n\n\nBackscatter from elevated buildings (double bounce)\n\n\n\n\n\n\n\nMultiple diffuse backscattering from objects with several surface layers \n\n\n\n\n\nPrevoius\n\n\n\nNext",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>SAR</span>"
    ]
  }
]