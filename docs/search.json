[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction to Remote Sensing",
    "section": "",
    "text": "1.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W2_Xaringan.html",
    "href": "W2_Xaringan.html",
    "title": "2  Xaringan Tool",
    "section": "",
    "text": "2.1 Presentation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Xaringan Tool</span>"
    ]
  },
  {
    "objectID": "W2_Xaringan.html#reflections-on-xaringan",
    "href": "W2_Xaringan.html#reflections-on-xaringan",
    "title": "2  Xaringan Tool",
    "section": "2.2 Reflections on Xaringan",
    "text": "2.2 Reflections on Xaringan\n\nSince taking CASA005 last term, I have been curious about how course materials are created. I often find myself sharing presentations via email, only to have to resend them after updates, which made me eager to try Xaringan! Initially, I felt a bit disappointed to learn that it is an R-based package, as I’m not the biggest fan of R and don’t find it particularly intuitive—mostly because I’ve used it primarily for statistical analysis in the past. However, I feel more comfortable with R now, having used it in various courses.\nWhen I started building my slides, the process was challenging. I attempted to follow the suggested Markdown structure, but it felt restrictive compared to the general HTML I was accustomed to. As a result, I found myself using HTML components for elements that were difficult to manage in Markdown. I’m unsure if this is considered bad practice or acceptable as long as it works!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Xaringan Tool</span>"
    ]
  },
  {
    "objectID": "W3_Corrections.html",
    "href": "W3_Corrections.html",
    "title": "3  Corrections and Enhancements",
    "section": "",
    "text": "3.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "W3_Corrections.html#summary",
    "href": "W3_Corrections.html#summary",
    "title": "3  Corrections and Enhancements",
    "section": "",
    "text": "This week, the lecture covered the essentials of preparing and enhancing satellite imagery for analysis, splitting the content into two main parts. The first part focuses on corrections and its different categories: geometric correction aligns distorted images using Ground Control Points (we discussed how to pick these points, noting that features like vegetation are poor choices since they change over time, making them unreliable for ensuring spatial accuracy); atmospheric correction removes haze and scattering effects through methods like Dark Object Subtraction or radiative transfer models; topographic correction adjusts for terrain distortions using elevation models ; and radiometric correction converts Digital Numbers to spectral radiance with gain and bias. The second part covers joining datasets and enhancements: Mosaicking combines overlapping images using feathering to blend them smoothly, adjusting brightness so the edges match seamlessly. Enhancements include contrast stretching to amplify visibility, using band ratios to show things like vegetation more clearly, and applying Principal Component Analysis to simplify complicated data into essential components.\nWith that being said, it’s important to mention that remote sensing products now often come pre-corrected and enhanced. Yet, understanding these processes remains crucial, as it empowers users to evaluate data quality, address specific project needs, and adjust workflows across diverse satellite sources. As I was trying to understand Analysis Ready Data (ARD), this article helped me understand ARD’s core processing concepts. In addition, this week’s practical highlighted how each satellite offers unique products, and how collections—systematically organized groups of satellite imagery—are categorized into different processing levels, which indicate the degree of preprocessing applied to the data. For instance, Level 1 data includes raw or minimally processed imagery, while higher levels, like Level 3, provide specialized outputs such as vegetation indices or land cover classifications, ready for direct use in applications (Figure 1). ARD brings clear benefits, including time savings, consistency, and accessibility that lowers technical barriers for users. However, its standardized processing may not account for regional variations, such as unique atmospheric conditions or terrain complexities, potentially reducing accuracy in specific contexts. Also, reliance on ARD can lead to a lack of transparency in the preprocessing steps, making it difficult for users to assess or modify the corrections applied. Thus, while ARD streamlines workflows, it is not a one-size-fits-all solution and recognizing its limitations is key to leveraging it effectively.\n\n\n\nFigure 1 - From left to right: (1) Sentinel-2 Level-1C TOA reflectance input image, (2) the atmospherically corrected Level-2A surface reflectance image, (3) the output scene classification of the Level-1C product - Source: European Space Agency, link\n\n\n\n\n3.1.1 Dictionary for some new terminologies I learned this week\n\n\n\n\n\n\n\n\n\nTerminology\nMeaning\n\n\n\n\nAnalysis Ready Data\nPre-processed and formatted data that is immediately usable for analysis.\n\n\nAtmospheric attenuation\nThe reduction of signal strength as electromagnetic waves pass through the atmosphere.\n\n\nFeathering\nA technique used to blend the edges of images or data from different sources to create a seamless appearance.\n\n\nImage fusion\nWhen data from multiple sensors/sources is fused together.\n\n\nIrradiance\nDownwelling radiation reaching the Earth from the sun.\n\n\nMosaicking\nThe process of combining multiple satellite images into a single, larger image.\n\n\nNadir\nThe point on the ground directly beneath a satellite or sensor.\n\n\nPath radiance\nRadiance reflected above the surface (before reaching the sensor).\n\n\nPseudo-invariant Features\nAreas on the Earth’s surface that remain relatively unchanged over time.\n\n\nPush broom\nA sensor that moves in a straight line, continuously capturing data across a wide area.\n\n\nRadiance\nAny radiation leaving the Earth.\n\n\nSurface Reflectance\nProportion of light reflected by the Earth’s surface.\n\n\nSpectral radiance\nThe amount of light within a band from a sensor in the field of view (FOV).\n\n\nSolar Azimuth\nThe angle between the sun and true north.\n\n\nSolar Zenith Angle\nThe angle between the local zenith (directly above) and the sun.\n\n\nWhisk broom\nA sensor that scans side to side, capturing data line by line.\n\n\n\n\n\n* Note: Definitions are based on my understanding, lecture notes, and several academic articles.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "W3_Corrections.html#applications",
    "href": "W3_Corrections.html#applications",
    "title": "3  Corrections and Enhancements",
    "section": "3.2 Applications",
    "text": "3.2 Applications\n\nIn this section, I wanted to focus on a specific application and discuss the correction technique used with its evolution over the years. Vegetation monitoring is one of the key applications where correction techniques have evolved significantly over the years, driven by advancements in remote sensing and data processing. Vermote et al. (1997) introduced the 6S (Second Simulation of the Satellite Signal in the Solar Spectrum) model, a radiative transfer code that uses inputs like aerosol optical depth, and water vapor to correct Landsat top-of-atmosphere (TOA) radiance to surface reflectance, enabling accurate NDVI for forest monitoring in regions like the Pacific Northwest. The paper confirms its accuracy through comparisons with other codes, achieving relative errors below 5% in reflectance under standard conditions. Hansen et al. (2008) propose an enhanced method integrating MODIS-derived atmospheric data with Landsat imagery, correcting for haze and clouds to monitor forest cover changes in the Congo Basin. More recently, Basener and Basener (2023) proposed a machine learning approach, training Gaussian Process and deep learning models on 100,000 MODTRAN-simulated spectra to correct hyperspectral imagery. By treating atmospheric interference as noise and directly applying radiative transfer physics, their technique improves adaptability to varying environmental conditions, ultimately enhancing NDVI precision.\nWhile 6S offers a robust, standardized baseline, Hansen’s MODIS-Landsat synergy improves large-scale forest monitoring but relies on coarse MODIS inputs, and Basener and Basener’s ML approach excels in flexibility yet demands computational resources. This progression—from physics-based simulation to multi-sensor integration and ML adaptability— highlight how the field is constantly evolving.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "W3_Corrections.html#reflections",
    "href": "W3_Corrections.html#reflections",
    "title": "3  Corrections and Enhancements",
    "section": "3.3 Reflections",
    "text": "3.3 Reflections\n\nReading more on the techniques covered this week made me reflect on how, while the convenience of pre-processed data is undeniable, it’s worth critically questioning whether these standardized corrections and enhancements are universally applicable. For example, atmospheric correction methods designed for one region might not perform as well in another due to differences in environmental conditions. Similarly, relying solely on higher-level products like vegetation indices could overlook nuances in the raw data that might be critical for specific analyses. For instance, Huete et al. (2002) caution that indices like NDVI can be less reliable in arid or semi-arid regions due to soil background effects, emphasizing the need for region-specific adjustments.\nCloud cover is often cited as one of the main challenges in remote sensing studies, but in the region I come from, the Middle East, I don’t think it’s as critical, and I see sandstorms as another key challenge for the region. Sandstorms introduce high levels of particulate matter into the atmosphere, which can distort spectral signals and reduce the accuracy of standard correction methods. This issue is less frequently addressed in mainstream literature. This gap in the literature is something I plan to explore further to better understand how sandstorms impact remote sensing data and how we can develop more robust correction methods for such environments. Moreover, Standardized approaches provide a foundation, but independent thinking and the ability to tailor preprocessing steps to specific project requirements are essential for ensuring the accuracy and reliability of remote sensing analyses.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "W3_Corrections.html#references",
    "href": "W3_Corrections.html#references",
    "title": "3  Corrections and Enhancements",
    "section": "3.4 References",
    "text": "3.4 References\n\nBasener, B., & Basener, A. (2023). Gaussian Process and Deep Learning Atmospheric Correction. Remote Sensing (Basel, Switzerland), 15(3), 649-. https://doi.org/10.3390/rs15030649\nHansen, M. C., Roy, D. P., Lindquist, E., Adusei, B., Justice, C. O., & Altstatt, A. (2008). A method for integrating MODIS and Landsat data for systematic monitoring of forest cover and change in the Congo Basin. Remote Sensing of Environment, 112(5), 2495–2513. https://doi.org/10.1016/j.rse.2007.11.012\nHuete, A., Didan, K., Miura, T., Rodriguez, E. P., Gao, X., & Ferreira, L. G. (2002). Overview of the radiometric and biophysical performance of the MODIS vegetation indices. Remote Sensing of Environment, 83(1), 195–213. https://doi.org/10.1016/S0034-4257(02)00096-2\nVermote, E. F., Tanre, D., Deuze, J. L., Herman, M., & Morcette, J.-J. (1997). Second Simulation of the Satellite Signal in the Solar Spectrum, 6S: an overview. IEEE Transactions on Geoscience and Remote Sensing, 35(3), 675–686. https://doi.org/10.1109/36.581987",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html",
    "href": "W4_Policy.html",
    "title": "4  EO Data in Policy Making",
    "section": "",
    "text": "4.1 Green Riyadh Initiative\nThis week, we explored how Earth Observation (EO) data can support policymaking. Below is a usecase illustrating how EO data can address a metropolitan policy challenge in Riyadh.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EO Data in Policy Making</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#green-riyadh-initiative",
    "href": "W4_Policy.html#green-riyadh-initiative",
    "title": "4  EO Data in Policy Making",
    "section": "",
    "text": "The Green Riyadh Initiative is a cornerstone of Saudi Arabia’s Vision 2030, aiming to transform Riyadh into a greener, more sustainable city. The project plans to plant 7.5 million trees across the city, increasing green cover from 1.5% to 9.1% by 2030. It seeks to combat urban heat, improve air quality, and enhance biodiversity, creating a healthier, more sustainable city.\nGrowing up in Riyadh, I’ve witnessed firsthand how the city’s extreme heat and urban sprawl have intensified over time. During peak summer, it’s not uncommon to see 45°C or higher, and the urban heat island (UHI) effect makes certain areas feel even hotter. Studies have shown that the UHI effect can increase temperatures by 4-6°C in densely built areas, and at its peak, it may exceed 10°C (Santamouris & Vasilakopoulou, 2023), exacerbating the city’s heat challenges.\nAir quality is another pressing issue, with high levels of particulate matter (PM2.5) and other pollutants, largely due to Riyadh’s car-centric urban design. Urban greenery has been proven to mitigate these challenges by providing shade, reducing car dependency, and absorbing pollutants (Gössling, 2020). By creating shaded walkways and green corridors, the initiative encourages walking and cycling, offering a sustainable alternative to short car trips.\nSince its launch in 2019 by the Royal Commission for Riyadh City (RCRC), the initiative has made significant progress. New parks and green corridors are emerging in neighborhoods, and trees now line almost all major highways. The RCRC has designed a master plan identifying key areas for afforestation, including:\n\n3,330 neighborhood gardens\n2,000 car parking sites\n16,400 Kilometers of streets and roads\n272 Kilometers of valleys\n175,000 Square Kilometers of empty land\n\nRCRC has also established clear guidelines for selecting plant species suitable for Riyadh’s climate and urban environment, prioritizing drought-resistant and native species to ensure sustainability.\nThe initiative aligns with national goals under Vision 2030, including the Saudi Green Initiative, which aims to plant 10 billion trees nationwide, guided by three overarching targets: emissions reduction, afforestation and land regeneration, and land and sea protection. By increasing green spaces, Riyadh contributes directly to these targets, promoting environmental sustainability and improving quality of life. It also connects to global goals, such as the United Nations Sustainable Development Goals (SDGs), particularly SDG 11 (Sustainable Cities and Communities), SDG 13 (Climate Action), and SDG 15 (Life on Land).\n\n\nBelow is a video that gives an overview about the the initiative:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EO Data in Policy Making</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#use-of-eo-data",
    "href": "W4_Policy.html#use-of-eo-data",
    "title": "4  EO Data in Policy Making",
    "section": "4.2 Use of EO Data",
    "text": "4.2 Use of EO Data\n\nIn this section, I’ll focus on how remotely sensed data can support a key objective of the Green Riyadh Initiative: reducing urban heat. While the initiative’s published documents do not explicitly mention the use of EO data, the reported progress suggests that change detection and monitoring are likely being employed. The initiative identifies specific areas and priorities for afforestation, with targeted numbers for different categories such as neighborhood gardens, parking sites, and valleys.\nThermal imagery from satellites like Landsat 8 and Sentinel-3 is one key application to evaluate land surface temperature (LST) reductions after tree planting (Figure 1), as demonstrated in studies like Sadek et al. (2020) and Xu et al. (2023). However, incorporating LiDAR and high-resolution multispectral data (e.g., from Sentinel-2) can take this a step further by enabling precise microclimate mapping. LiDAR’s ability to map canopy height (link) and density at resolutions down to 1 meter, paired with Sentinel-2’s 10-meter multispectral bands could model localized climate conditions at a granular level.  By processing Sentinel-2 data to calculate vegetation indices like the Normalized Difference Vegetation Index (NDVI) and Enhanced Vegetation Index (EVI), planners can monitor the health, photosynthesis, and transpiration of Riyadh’s urban greenery. These indices, when overlaid with canopy maps, could generate microclimate maps allowing for targeted afforestation efforts where tree planting can have the most significant cooling impact.\n\n\n\nFigure 1 - Seasonal and spatial variability of LST in Riyadh - Source: Alghamdi et al. 2021\n\n\nTo enrich this microclimate mapping, datasets from the Copernicus Climate Change Service (C3S) offer a broader climatic lens. The C3S ERA5-Land dataset (link), delivering hourly and monthly variables such as 2-meter air temperature, soil moisture, and evapotranspiration at 9-km resolution, can be downscaled and fused with Sentinel-2 NDVI to reveal, for instance, how afforestation along Riyadh’s wadis lowers near-surface air temperature by 1-2°C while boosting humidity in dry seasons, tailoring planting strategies to maximize cooling. Complementing this, the C3S Seasonal Forecast dataset (link) (e.g., ECMWF’s SEAS5 model) provides six-month predictions of temperature and precipitation anomalies, allowing policymakers to optimize planting schedules—perhaps prioritizing early spring in dust-prone areas like Diriyah to enhance tree establishment and microclimatic benefits.\nRelating this to the discussed study during lecture by MacLachlan et al. 2021, which emphasized the importance of strategically placing trees to optimize temperature reduction, microclimate mapping emerges as a vital tool for enhancing the effectiveness of the Green Riyadh Initiative. The initiative’s published documents, while rich with targets, lack the fine-scale spatial detail needed to prioritize plantings where heat mitigation is most urgent. Microclimate mapping, powered by EO data, fills this gap by identifying localized heat sinks offering a pathway to maximize ecological returns on investment. Without such granularity, the initiative risks diluting its cooling potential across less impactful sites, underscoring the transformative role EO-driven mapping could play in sharpening policy focus and execution.\n\n\n\nIllustration of the Green Riyadh Initiative’s greening efforts along a highway at the city’s outer boundary - Source: Green Riyadh, 2023",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EO Data in Policy Making</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#reflections",
    "href": "W4_Policy.html#reflections",
    "title": "4  EO Data in Policy Making",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\n\nWith the big transformation Riyadh is seeing these years, it was challenging to identify a single policy or a challenge that EO can support, but the Green Riyadh Initiative stands out for its citywide reach. Shaded walkways, new parks, and increased pedestrian activity are visible changes, but whether these efforts meaningfully reduce LST remains unclear without EO data. This highlights a critical gap: without leveraging remote sensing technologies, we cannot fully understand or measure the success of such initiatives. While researching I came across an intriguing finding from an urban scale study on one of Riyadh’s neighborhoods by Haddad et al. (2024). They highlighted how non-irrigated plants help cool cities at night but aren’t very effective during the day, whereas Irrigated trees are the best option for cooling cities both day and night. This aligns with micro-scale studies, such as Zölch et al. (2016), which emphasize the importance of detailed EO data in understanding localized climate impacts and optimizing urban greening strategies.\nOn a broader note, although not related to my use case, one of the key examples that highlight EO data’s power comes from the International Energy Agency (IEA). In 2022, they reported that satellite imagery revealed methane emissions from the energy sector to be 70% higher than official government reports. This underscores EO’s potential to provide accurate, actionable insights that can drive policy decisions. In conclusion, incorporating EO data into more policies isn’t just an enhancement—it’s essential for grounding decisions in evidence, ensuring initiatives like Green Riyadh deliver measurable benefits, and tackling global challenges with precision and accountability.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EO Data in Policy Making</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#references",
    "href": "W4_Policy.html#references",
    "title": "4  EO Data in Policy Making",
    "section": "4.4 References",
    "text": "4.4 References\n\nAlghamdi, A. S., Alzhrani, A. I., & Alanazi, H. H. (2021). Local Climate Zones and Thermal Characteristics in Riyadh City, Saudi Arabia. Remote Sensing, 13(22), 4526. https://doi.org/10.3390/rs13224526\nGössling, S. (2020). Why cities need to take road space from cars - and how this could be done. Journal of Urban Design, 25(4), 443–448. https://doi.org/10.1080/13574809.2020.1727318\nHaddad, S., Zhang, W., Paolini, R., Gao, K., Altheeb, M., Al Mogirah, A., Bin Moammar, A., Hong, T., Khan, A., Cartalis, C., Polydoros, A., & Santamouris, M. (2024). Quantifying the energy impact of heat mitigation technologies at the urban scale. Nature Cities, 1(1), 62–72. https://doi.org/10.1038/s44284-023-00005-5\nInternational Energy Agency. (2022, February 23). Methane emissions from the energy sector are 70% higher than official figures. https://www.iea.org/news/methane-emissions-from-the-energy-sector-are-70-higher-than-official-figures\nMacLachlan, A., Biggs, E., Roberts, G., & Boruff, B. (2021). Sustainable City Planning: A Data-Driven Approach for Mitigating Urban Heat.\nSadek, M., Beshr, A. A., Kaloop, M. R., Liu, G., Co, Y., Mustafa, E. K., Zarzoura, F., & Zhao, D. (2020). Study for Predicting Land Surface Temperature (LST) Using Landsat Data: A Comparison of Four Algorithms. Advances in Civil Engineering, 2020(2020), 1–16. https://doi.org/10.1155/2020/7363546\nSantamouris, M., & Vasilakopoulou, K. (2023). Recent progress on urban heat mitigation technologies. Science Talks (Online), 5, 100105-. https://doi.org/10.1016/j.sctalk.2022.100105\nXu, X., Pei, H., Wang, C., Xu, Q., Xie, H., Jin, Y., Feng, Y., Tong, X., & Xiao, C. (2023). Long-term analysis of the urban heat island effect using multisource Landsat images considering inter-class differences in land surface temperature products. The Science of the Total Environment, 858, 159777-. https://doi.org/10.1016/j.scitotenv.2022.159777\nZölch, T., Maderspacher, J., Wamsler, C., & Pauleit, S. (2016). Using green infrastructure for urban climate-proofing: An evaluation of heat mitigation measures at the micro-scale. Urban Forestry & Urban Greening, 20, 305–316. https://doi.org/10.1016/j.ufug.2016.09.011",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EO Data in Policy Making</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html",
    "href": "W6_GEE.html",
    "title": "5  Google Earth Engine (GEE)",
    "section": "",
    "text": "5.1 Introduction to GEE",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#introduction-to-gee",
    "href": "W6_GEE.html#introduction-to-gee",
    "title": "5  Google Earth Engine (GEE)",
    "section": "",
    "text": "This week we were introduced to Google Earth Engine (GEE), the one-hour lecture was heavy, full of new terminologies and concept. The session covered: code syntax, objects, and functions; the differences between client-side and server-side processes; and key technical ideas like scaling, projections, and reductions. We also explored common analytical techniques, including zonal statistics, regression, and principal component analysis. \nHere, the focus will be on two key components that i think are essential building blocks for anyone beginning to work with Google Earth Engine (GEE): scaling and reduction\nIn GEE, the concept of scale refers to the resolution of the data—how large or small each pixel is, such as 10 meters, 30 meters, or 500 meters. Unlike traditional GIS tools, where the resolution is determined by the input data, GEE takes a distinctive approach by tying scale to the output. This process occurs seamlessly on the server side, enabling users to handle datasets with varying native resolutions without manual resizing, reprojection, or alignment. If the scale isn’t explicitly set, GEE assigns one based on the context—typically the native resolution of the input data or the zoom level of the map in view. This flexibility stands out as one of GEE’s greatest strengths, removing the time-consuming preprocessing steps common in traditional workflows. Yet, this convenience brings a layer of responsibility: users must thoughtfully choose a scale that fits their analysis needs. A scale too coarse risks hiding critical patterns or details, while one too fine can make the analysis overly complex or computationally intensive.\nReduction is another fundamental concept that plays a crucial role in analyzing and summarizing geospatial data. It refers to aggregating pixel values into concise statistics, such as means, sums, or extremes, enabling analysis across spatial or temporal dimensions. For example, reducing a time series of satellite images to a single composite can reveal trends in vegetation health (e.g., NDVI) or land surface temperature over time (Zhang et al., 2003). Reduction techniques in GEE can be broadly categorized into:\n\nTemporal Reduction (imageCollection.reduce()): Aggregating data over time, such as calculating seasonal or annual averages. This is particularly useful for monitoring long-term environmental changes .\nSpatial Reduction:\n• reduceRegion(), reduceRegions() - (Figure 1): Summarizing data over a geographic area to compute statistics for polygons or points.\n• reduceNeighborhood() - (Figure 2): Aggregating data within a kernel or moving window, which is useful for smoothing data or detecting spatial patterns (e.g., edge detection or texture analysis).\nSpectral Reduction (image.reduce()): Combining or summarizing data across multiple bands, such as computing vegetation indices or band-specific statistics.\n\n\n\n\n\nFigure 1 - Illustration of a Reducer applied to an ImageCollection.  Source\n\n\n\n\n\nFigure 2 - Illustration of reduceNeighborhood(), where the reducer is applied in a kernel  Source\n\n\n\nScale and reduction are foundational to geospatial analysis in GEE: scale defines the resolution, shaping the framework for analysis, while reduction extracts meaningful insights from data. Mastering these concepts goes beyond technical skill—it’s about balancing detail and simplicity to address real-world questions effectively. I came across a very interesting metaphor in a post (unfortunately, I’ve lost the link, but it’s too good to skip): it’s almost like cooking— scale preps the ingredients, chopping them just right, and reduction simmers them into a perfect sauce. Get one wrong, and the whole dish is off. In GEE, this balance feels like an art form, one that takes practice to master but opens up a world of possibilities once you do. So it’s not just about the tools; it’s about understanding the process, experimenting, and refining your approach.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#applications",
    "href": "W6_GEE.html#applications",
    "title": "5  Google Earth Engine (GEE)",
    "section": "5.2 Applications",
    "text": "5.2 Applications\n\nGEE has emerged as a critical tool for accessing and analyzing geospatial data across various disciplines. Pham-Duc et al. (2023) reviewed trends in GEE usage by analyzing peer-reviewed articles up to 2023, revealing that nearly 50% of applications are concentrated in the fields of Earth and planetary sciences and environmental science (Figure 3). This dominance aligns with these fields’ reliance on processes that are inherently spatial, temporal, and data-intensive. GEE, with its extensive satellite imagery archive, cloud-based processing power, and real-time capabilities, is ideally suited for studying phenomena such as deforestation, climate change, and ecosystem dynamics.\n\n\n\nFigure 3 - Disciplines where GEE has been applied\n\n\nGlobal Forest Cover Change by Hansen et al. (2013) stands out as one of the pioneering applications in quantifying global forest extent and change in a high-resolution (30-meter) thematic map. This work has become a cornerstone for researchers studying deforestation and forest dynamics at a global scale (Heino et al., 2015; Tyukavina et al., 2017). In addition, GEE has played a critical role in real-time disaster monitoring, such as during the recent California wildfires, where it enabled rapid mapping of fire perimeters, burn severity, and smoke dispersion using satellite data (Anderson, 2025).\nGEE’s potential extends beyond environmental and disaster applications. By providing access to high-performance computing and vast datasets, it empowers researchers and policymakers to address global challenges with remarkable precision and the ability to scale up solutions. Yet, the platform’s pre-processed datasets and proprietary algorithms, while efficient, may obscure methodological details, complicating replication and potentially reducing trust in results for applications needing rigorous validation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#reflections",
    "href": "W6_GEE.html#reflections",
    "title": "5  Google Earth Engine (GEE)",
    "section": "5.3 Reflections",
    "text": "5.3 Reflections\n\nThis week, one main thought that continually came to my mind while learning more about GEE is why it hasn’t been more widely adopted by governments and large companies, especially given its real-time capabilities and open-access model. While GEE is still popular among researchers and environmental scientists for its ability to process vast amounts of satellite data quickly, its reliance on coding (JavaScript or Python) can be a significant barrier for non-technical users. In contrast, Esri’s ArcGIS dominates in global policy and spatially based applications, offering a comprehensive, enterprise-grade GIS suite with user-friendly interfaces like ArcGIS Pro and ArcGIS Online. These features make Esri more accessible for policymakers and organizations that may lack technical expertise but require robust tools for decision-making. However, Esri’s dominance raises concerns about accessibility for underfunded regions and institutions that cannot afford proprietary software licenses. GEE’s open-access model has the potential to democratize geospatial analysis, particularly in developing countries or smaller organizations, but its steep learning curve and lack of intuitive graphical interfaces limit its broader adoption. Additionally, GEE’s cloud-based nature, while powerful, can be a limitation for users handling sensitive data that requires offline processing. That said, despite GEE’s established collaborations with the United Nations and NGOs—such as UNEP and FAO for environmental monitoring and disaster response—I think its niche as a coding-heavy, research-focused tool limits its appeal for widespread governmental and corporate use compared to Esri’s enterprise-ready solutions. GEE could expand its influence by enhancing user-friendly interfaces, integrating AI automation, and forming deeper enterprise partnerships, potentially complementing Esri’s strengths to create a more inclusive geospatial ecosystem for global policy and decision-making.\nPersonally, I haven’t used GEE directly, as my employer relies on an enterprise Esri solution that provides a robust GIS framework for our geospatial needs. However, I’ve indirectly utilized its data catalogue by accessing satellite imagery processed through GEE and integrated into our Esri workflows.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#references",
    "href": "W6_GEE.html#references",
    "title": "5  Google Earth Engine (GEE)",
    "section": "5.4 References",
    "text": "5.4 References\n\nGoogle Earth Engine Team. (n.d.). Introduction to reducers. Google Developers. https://developers.google.com/earth-engine/guides/reducers_intro\nHansen, M., Potapov, P., Moore, R., Hancher, M. (2013). The first detailed maps of global forest change. Google Research Blog. Retrieved from https://research.google/blog/the-first-detailed-maps-of-global-forest-change/\nHeino, M., Kummu, M., Makkonen, M., Mulligan, M., Verburg, P. H., Jalava, M., & Räsänen, T. A. (2015). Forest loss in protected areas and intact forest landscapes: A global analysis. PLOS ONE, 10(10), e0138918. https://doi.org/10.1371/journal.pone.0138918\nJohn Anderson (2025). Supporting the Los Angeles Community During the Wildfires. Google Earth. Retrieved from https://medium.com/google-earth/supporting-the-los-angeles-community-during-the-wildfires-f67614046794\nPham-Duc, B., Nguyen, H., Phan, H., & Tran-Anh, Q. (2023). Trends and applications of Google Earth Engine in remote sensing and earth science research; a bibliometric analysis using SCOPUS database. Earth Science Informatics, 16(3), 2355–2371. https://doi.org/10.1007/s12145-023-01035-2\nTyukavina, A., Hansen, M. C., Potapov, P. V., Stehman, S. V., Smith-Rodriguez, K., Okpa, C., & Aguilar, R. (2017). Types and rates of forest disturbance in Brazilian Legal Amazon, 2000–2013. Science Advances, 3(4), e1601047–e1601047. https://doi.org/10.1126/sciadv.1601047\nZhang, X., Friedl, M. A., Schaaf, C. B., Strahler, A. H., Hodges, J. C. F., Gao, F., Reed, B. C., & Huete, A. (2003). Monitoring vegetation phenology using MODIS. Remote Sensing of Environment, 84(3), 471–475. https://doi.org/10.1016/S0034-4257(02)00135-9",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA00023 Learning Diary",
    "section": "",
    "text": "My Learning Journey!\n\nWelcome to my learning diary, where I will reflect on my weekly engagement with course content, readings, and practical materials using Quarto in R. Each book will highlight key insights and applications of the concepts discussed, capturing what I have learned and found interesting each week. The structure of my diary includes a summary of the weekly material, an exploration of how these concepts have been applied in literature or policy, and personal reflections that contextualize the skills and knowledge gained, considering their relevance and potential future applications based on my interests.\nMy name is Norah Alhosain, and I am from Riyadh, Saudi Arabia. I joined this master’s program with the goal of enhancing my knowledge and building on my existing experience. For about three years, I have been considering pursuing a master’s degree and actively searching for a relevant program. When I found that UCL was merging the Smart Cities and Urban Analytics with Spatial Data Science and Visualization programs, I felt confident that this was exactly what I was looking for. I have a background in computer science and several years of work experience at an advisory think tank in Riyadh, focusing on energy, sustainability, and economics. In my role, I’ve done lots for spatial analysis spatial analysis, particularly in transportation and network optimization, but I haven’t yet explored remote sensing in depth. This gap fuels my enthusiasm to learn and enhance my expertise in this area through this program. Here are some visual results from my previous analyses, which primarily focus on transportation challenges in Riyadh.\n\n\n\n    \n    \n       Riyadh traffic density throughout the day  \n    \n\n\n    \n    \n        Accessibility to Riyadh bus network stations",
    "crumbs": [
      "My Learning Journey!"
    ]
  },
  {
    "objectID": "W7_classificarion_1.html",
    "href": "W7_classificarion_1.html",
    "title": "6  Classification I",
    "section": "",
    "text": "6.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification I</span>"
    ]
  },
  {
    "objectID": "W7_classificarion_1.html#summary",
    "href": "W7_classificarion_1.html#summary",
    "title": "6  Classification I",
    "section": "",
    "text": "This week, we explored a foundational concept in remote sensing: classification, the process of categorizing image pixels into distinct classes based on their spectral characteristics. The first part of the lecture covered real-world applications like tracking urban sprawl, linking air pollution to land use, and pinpointing deforestation hotspots. The forest monitoring and illegal logging case was really interesting. In that case, Landsat imagery was pre-processed—resampled, converted into top-of-atmosphere reflectance, and cleared of cloud cover—before introducing a key concept: “creating metrics”. This essential step, performed before classification, involves deriving specific numerical measures or indices (e.g., maximum/minimum reflectance, mean values, or temporal slopes) from spectral bands across a time series. These metrics transform raw data into a “feature space,” allowing algorithms to identify patterns, such as the detection of 5,000 deforestation sites in Brazil (Hansen et al., 2013).\nThe second part shifted focus to how classification is done, covering general machine learning algorithms (e.g., linear regression, random forest) and how they are applied in a spatial context. Linear regression is primarily utilized to estimate continuous measures, such as the distribution of atmospheric pollutants across geographic expanses, providing a foundational approach to predicting trends over space and time. Random Forests, conversely, assemble multiple decision trees that analyze random data samples to classify land types—forests, urban areas, or fields—enhancing accuracy through collective evaluation. This method leverages bootstrapping and random variable selection to enhance reliability, as demonstrated in its application to land cover mapping.\nThe lecture referenced Maximum Likelihood, an older probability-based method, but we did not cover it, as it is not used anymore. Looking at why, I learned how its reliance on assumptions of uniform data distribution is considered less effective for handling the diverse and complex imagery prevalent today, particularly given advancements in computational technology (Richards, 2006).\nDespite these advances, challenges and limitations persist. Decision trees risk overfitting, where models excessively conform to training data, reducing their ability to generalize. Mitigation through pruning or setting minimum leaf sizes addresses this, though it increases complexity and requires precise tuning. Additionally, the lecture highlights a trade-off between complexity and interpretability: advanced classifiers like Random Forests and Support Vector Machines offer high accuracy but might become “black boxes,” reducing interpretability.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification I</span>"
    ]
  },
  {
    "objectID": "W7_classificarion_1.html#applications",
    "href": "W7_classificarion_1.html#applications",
    "title": "6  Classification I",
    "section": "6.2 Applications",
    "text": "6.2 Applications\n\nWhile the summary emphasized algorithmic approaches (e.g., Random Forests (RF), Support Vector Machines (SVM)), another critical distinction in classification methods we covered this week lies in supervised and unsupervised techniques, each offering unique applications. Unsupervised approaches, like k-means clustering, identify natural groupings without prior labels, offering flexibility for exploratory tasks where the goal is discovery rather than prediction. On the other hand, Supervised classification, where algorithms like Random Forests or SVM are trained on labeled datasets, excels in precise, goal-driven tasks. Both algorithms, are supervised learning methods, but they differ significantly in their approach, data handling, and performance characteristics. Here I’ll focus on highlighting that:\n- SVM:\nSimilar to logistic regression, SVM minimizes generalization error without distribution assumptions, separating classes (e.g., forest vs. farmland) through an optimal hyperplane. It excels in high-dimensional applications, one example is detecting oil spills in synthetic aperture radar (SAR) imagery for environmental monitoring. Matkan et al. (2013) applied SVM to SAR images from the Gulf of Mexico, achieving high accuracy by training on texture and intensity features to differentiate oil-contaminated water from clean surfaces. Optimal performance required tuning parameters C (regularization) and gamma (kernel scale) through grid search and cross-validation, ensuring a balance between margin maximization and classification precision. However, this tuning process is computationally demanding, particularly for large datasets, and SVM’s limited interpretability—offering no direct insight into feature contributions—may restrict its use in applications requiring transparent decision-making, such as policy reporting for spill response strategies.\n- RF:\nThis methods ability to quantify feature importance makes it valuable for various applications. One example is its role in analyzing burn severity drivers, as demonstrated by Huang et al. (2020) in California’s coastal mountains. Their study used RF to model burn severity, identifying slope, aspect, fuel moisture, and long-term climate as key predictors. During droughts, RF revealed that low fuel moisture and high climatic water deficit doubled high-severity burn areas. However, while efficient and interpretable, this approach may oversimplify localized fire behavior, particularly in heterogeneous landscapes where small differences in terrain or fuel types affect fire intensity. Additionally, correlated features (e.g., temperature and drought indices) can distort importance rankings, requiring careful variable selection.\n\n\n\nFigure 1 - RF Vs SVM approach.  Source",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification I</span>"
    ]
  },
  {
    "objectID": "W7_classificarion_1.html#reflections",
    "href": "W7_classificarion_1.html#reflections",
    "title": "6  Classification I",
    "section": "6.3 Reflections",
    "text": "6.3 Reflections\n\nReflecting on this lecture, I found it easy to follow and connect with, probably because classification appears in many familiar research areas like city planning or detecting change. Before this, I had some knowledge of machine learning, but not within remote sensing.\nA few years ago, in 2020, I attempted to detect building footprints in Riyadh using an Esri machine learning tool. The results were inaccurate, and at the time, I assumed buildings shared similar traits like materials across regions. I didn’t fully understand how Riyadh’s buildings differ in spectral and structural characteristics from those in the tool’s training data and the big impact this has. Now, I see how methods like SVM classification, introduced in the lecture, could improve such work by adjusting to local details.\nThe lecture also raised a broader consideration: the tendency to utilize the most accurate tools without considering their suitability for a given problem. The lecture showed Maximum Likelihood isn’t used much anymore because it can’t keep up with today’s messy data (Richards, 2006), which makes me think: how often do we pick flashy tech over what actually works for the problem?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification I</span>"
    ]
  },
  {
    "objectID": "W7_classificarion_1.html#references",
    "href": "W7_classificarion_1.html#references",
    "title": "6  Classification I",
    "section": "6.4 References",
    "text": "6.4 References\n\nHansen, M.C., Potapov, P.V., Moore, R., Hancher, M., Turubanova, S.A., Tyukavina, A., Thau, D., Stehman, S.V., Goetz, S.J., Loveland, T.R., Kommareddy, A., Egorov, A., Chini, L., Justice, C.O., Townshend, J.R.G., 2013. High-Resolution Global Maps of 21st-Century Forest Cover Change. Science 342, 850–853.\nHuang, Y., Jin, Y., Schwartz, M. W., & Thorne, J. H. (2020). Intensified burn severity in California’s northern coastal mountains by drier climatic condition. Environmental Research Letters, 15(10), 104033-. https://doi.org/10.1088/1748-9326/aba6af\nMatkan, A. A., Hajeb, M., & Azarakhsh, Z. (2013). Oil spill detection from SAR image using SVM based classification. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences., XL-1/W3, 55–60. https://doi.org/10.5194/isprsarchives-XL-1-W3-55-2013\nRichards, J. A. (John A., & Jia, X. (2006). Remote sensing digital image analysis : an introduction. (4th ed. / John A. Richards, Xiuping Jia). Springer.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification I</span>"
    ]
  },
  {
    "objectID": "W8_classification_2.html",
    "href": "W8_classification_2.html",
    "title": "7  Classification II",
    "section": "",
    "text": "7.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "W8_classification_2.html#applications",
    "href": "W8_classification_2.html#applications",
    "title": "7  Classification II",
    "section": "7.2 Applications",
    "text": "7.2 Applications\n\n1. Pixel-Based Classification: Limitations in Urban Landscapes\nPixel-based methods classify individual pixels based solely on their spectral signatures. While effective for large-scale studies like the global deforestation mapping by Hansen et al. (2013), which was discussed in the lecture, they face challenges in urban environments, including:\n\n\nSpectral ambiguity between different urban materials\nPrevalence of mixed pixels in high-resolution imagery\nInability to incorporate spatial relationships between features\n\n\nThese limitations highlight a deeper issue: pixel-based methods assume spectral data alone can tell the full story, ignoring the spatial relationships that define urban complexity. This raises doubts about their suitability for tasks with high level of details, where material diversity and small-scale layouts challenge spectral clarity. Could over-reliance on this method lead to oversimplified conclusions in rapidly changing cities?\n2. Object-Based Classification: A Spatial-Spectral Solution\nObject-based image analysis (OBIA) addresses these limitations through a hierarchical approach that:\n\n\nSegments imagery into meaningful objects based on both spectral and spatial characteristics\nIncorporates contextual information through shape, texture, and relational metrics\nUtilizes multi-scale analysis to capture features at appropriate levels of detail\n\n\nThe efficacy of this approach was demonstrated by Myint et al. (2011), whose comparative analysis of land cover in Phoenix showed a significant accuracy improvement—90.4% for object-based classification versus 67.6% for pixel-based classification, as visually evidenced in their Figure 1 below. This finding underscores the effectiveness of incorporating spatial context in classification efforts. However, OBIA’s strength—its reliance on segmentation—also introduces vulnerabilities. Selecting the appropriate scale is subjective and risks inconsistency if poorly chosen. This subjectivity could undermine reliability in large-scale projects, where uniform standards are essential. Moreover, the added computational demand of OBIA prompt the question: does its accuracy justify the resource cost, particularly in settings with limited resources where simpler methods might be sufficient?\n\n\n\nFigure 1 - (a) Test image; (b) output map produced by the object-based approach; (c) output map produced by the classical per-pixel classifier. Note: Cyan = buildings; orange = unmanaged soil; light green = grass; gray = other impervious surfaces, purple = swimming pools; dark green = trees and shrubs; and blue = lakes and ponds. - Source: (Myint et al., 2011)\n\n\n3. Current Trends and Future Directions\nRecent studies suggest that hybrid approaches, such as using OBIA for segmentation followed by Random Forest classification, can enhance classification outcomes (Ma et al., 2017). This integration of methodologies represents a promising direction, combining the strengths of both pixel-based and object-based approaches to improve accuracy and reliability.\nAdditionally, emerging trends, including deep learning Convolutional Neural Networks (CNNs), further complicate these paradigms by automating spatial feature extraction. This advancement enables pixel-object fusion for applications such as slum mapping and wetland delineation (Zhang et al., 2019). The potential of these technologies to revolutionize remote sensing analysis is significant, suggesting a future where classification methods are more adaptive and sophisticated.\nThis methodological evolution has established object-based approaches as the standard for urban remote sensing applications. However, the choice between pixel-based and object-based methods ultimately depends on specific research objectives, the scale of analysis, and available computational resources.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "W8_classification_2.html#summary",
    "href": "W8_classification_2.html#summary",
    "title": "7  Classification II",
    "section": "",
    "text": "This week’s lecture is a continuation of classification related concepts, it was condensed, but I’ll try to briefly summarize main ideas grouped into the following sections:\n\n\n7.1.1 Advanced Analytical Techniques\n\nRelated content is explained in detail in the upcoming application section, where it makes sense to present it alongside practical examples. I opted not to explain it here to avoid redundancy.\n\n\n\n7.1.2 Evaluation and Performance Metrics\n\nIn machine learning, accuracy assessment is essential for validating model outputs. For remote sensing applications, this typically focuses on Producer Accuracy (PA) and User Accuracy (UA), as shown in Figure 1. These metrics are complemented by Overall Accuracy (OA), which aggregates correctly classified pixels across all categories. The F1 score provides a balanced measure between PA and UA, ranging from 0 (poor performance) to 1 (perfect balance). The Receiver Operating Characteristic curve (ROC) offers another perspective by plotting true versus false positive rates, with the Area Under the Curve (AUC) quantifying performance from 0.5 (random) to 1 (optimal).\nWhile these metrics work well with balanced data, the lecture highlighted how imbalances are common. Additionally, the equal weighting of PA and UA in the F1 score may not suit all applications, like urban mapping, where false negatives—missing urban pixels—matter more than false positives. For ROC in the other hand, the binary focus may oversimplify multi-class classifications.\n\n\n\nFigure 1 - Producer and User Accuracy (Source: Banko et al., 1998)\n\n\n\n\n\n7.1.3 Testing Data Selection and Spatial Validation\n\nTo reinforce the accuracy metrics outlined in the previous section, different validation strategies can be used. The most straightforward approach—a simple train-test split—reserves a subset of the data for final evaluation. More rigorous methods, like k-fold cross-validation, cycle through multiple data partitions, training the model on all but one-fold and testing on the remaining one.\nBut spatial data has a unique issue: spatial autocorrelation, where nearby locations tend to be similar—which means traditional validation can dramatically overestimate performance (Tobler, 1970). Spatial cross-validation methods address this by enforcing geographic separation between training and test sets. However, lack of clear rules for how much spatial separation is enough introduces uncertainty. Current best practices recommend spatially blocked designs that maintain realistic independence while remaining computationally feasible (Roberts et al., 2017), though the choice ultimately depends on each project’s specific spatial structure and resource constraints\n\n“Everything is related to everything else, but near things are more related than distant things”\n— Tobler, 1970",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "W8_classification_2.html#reflections",
    "href": "W8_classification_2.html#reflections",
    "title": "7  Classification II",
    "section": "7.3 Reflections",
    "text": "7.3 Reflections\n\nThe tension between methodological rigor and practical implementation proves especially significant in emerging fields like climate resilience planning. While spatially blocked cross-validation effectively addresses spatial autocorrelation (Roberts et al., 2017), its static geographic partitions may not adequately capture dynamic systems where spatial relationships evolve over time.\nThis limitation highlights a broader conceptual shift in geospatial analysis—from purely statistical validation toward impact-aware evaluation frameworks. The prioritization of specific error types (e.g., false negatives in urban mapping) extends beyond technical considerations to reflect fundamental value judgments about risk assessment. For example: • Environmental justice applications may require differential weighting of errors, where underestimating pollution exposure (false negatives) carries greater consequences than misclassifying green spaces • Disaster response systems could employ adaptive significance metrics, with error costs scaling dynamically based on event likelihood\nThese observations resonate particularly with my planned dissertation work on detection methodologies. The choice between pixel-based and object-based approaches—a fundamental analytical decision—directly influences detectable patterns and outcomes. This realization underscores the need for deeper investigation into how methodological selections shape research findings across different applications",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "W8_classification_2.html#references",
    "href": "W8_classification_2.html#references",
    "title": "7  Classification II",
    "section": "7.4 References",
    "text": "7.4 References\n\nBanko, Gebhard. (1998). A Review of Assessing the Accuracy of Classifications of Remotely Sensed Data and of Methods Including Remote Sensing Data in Forest Inventory.\nBrenning, A. (2012). Spatial cross-validation and bootstrap for the assessment of prediction rules in remote sensing: The R package sperrorest. 2012 IEEE International Geoscience and Remote Sensing Symposium, 5372–5375. https://doi.org/10.1109/IGARSS.2012.6352393\nMa, L., Li, M., Ma, X., Cheng, L., Du, P., & Liu, Y. (2017). A review of supervised object-based land-cover image classification. ISPRS Journal of Photogrammetry and Remote Sensing, 130, 277–293. https://doi.org/10.1016/j.isprsjprs.2017.06.001\nMyint, S. W., Gober, P., Brazel, A., Grossman-Clarke, S., & Weng, Q. (2011). Per-pixel vs. object-based classification of urban land cover extraction using high spatial resolution imagery. Remote Sensing of Environment, 115(5), 1145–1161. https://doi.org/10.1016/j.rse.2010.12.017\nRoberts, D. R., Bahn, V., Ciuti, S., Boyce, M. S., Elith, J., Guillera‐Arroita, G., Hauenstein, S., Lahoz‐Monfort, J. J., Schröder, B., Thuiller, W., Warton, D. I., Wintle, B. A., Hartig, F., & Dormann, C. F. (2017). Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure. Ecography (Copenhagen), 40(8), 913–929. https://doi.org/10.1111/ecog.02881\nTobler, W. R. (1970). A Computer Movie Simulating Urban Growth in the Detroit Region. Economic Geography, 46, 234–240. https://doi.org/10.2307/143141\nZhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., & Atkinson, P. M. (2019). Joint Deep Learning for land cover and land use classification. Remote Sensing of Environment, 221, 173–187. https://doi.org/10.1016/j.rse.2018.11.014",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html",
    "href": "W9_SAR.html",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "",
    "text": "8.1 Summary\nFigure 2 - SAR Backscatter Mechanisms - Source: [EUSI]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperture Radar (SAR)</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#summary",
    "href": "W9_SAR.html#summary",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "",
    "text": "In the introduction page of this diary, I briefly discussed the distinction between active and passive sensors. However, much of this module has focused on passive optical sensors. This week, we concluded the module by shifting our attention to Synthetic Aperture Radar (SAR) - an active technology with unique advantages: day/night operation (independent of solar illumination), all-weather capability, subsurface penetration potential, and flexible imaging configurations ranging from wide-area coverage to high-resolution mapping.\n\n\n8.1.1 How does SAR Work?\nSAR emits radiofrequency pulses toward Earth’s surface from a moving platform, like a satellite. These radar waves bounce back after interacting with features below, carrying information about their size, orientation, composition, and texture. In radar systems, resolution improves with a larger antenna aperture, much like how a bigger aperture in optical cameras boosts light collection for sharper images. Yet, for a satellite to achieve high-resolution Earth imaging, a physical antenna would need to be impractically massive—spanning hundreds of meters. SAR overcomes this limitation by “synthesizing” a large aperture using motion. As the satellite travels along its orbit, it transmits multiple radar pulses and records their reflections (Figure 1) . By combining these signals over time, SAR simulates the effect of a massive antenna, achieving high resolution without the need for an impractical physical structure. While groundbreaking, this approach has inherent limitations. The technique’s dependence on precise motion makes it sensitive to platform stability - even minor deviations in the satellite’s trajectory can compromise the synthesized aperture, potentially degrading image quality. Additionally, the complex signal processing demands substantial computational resources and introduces geometric distortions that require correction.\n\n\n\nFigure 1 - Depiction of synthetically increasing aperture for a given sensor and using concepts from camera optics to analogize the processes. - Source: Esri\n\n\n\n\nThe reflections SAR captures, known as “backscatter,” vary by scattering mechanism, revealing insights into the surface type of an object (Figure 2). However, surface properties are complicated, Campbell (2002) highlights how these complexities create an “inverse problem” where multiple surface conditions may generate identical backscatter responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBackscatter from a rough surface (single bounce)\n\n\n\n\n\n\n\nBackscatter from a smooth surface (single bounce)\n\n\n\n\n\n\n\nBackscatter from elevated buildings (double bounce)\n\n\n\n\n\n\n\nMultiple diffuse backscattering from objects with several surface layers \n\n\n\n\n\nPrevoius\n\n\n\nNext",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperture Radar (SAR)</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#applications",
    "href": "W9_SAR.html#applications",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "8.2 Applications",
    "text": "8.2 Applications\n\nSAR applications are diverse, ranging from change detection, disaster response, environmental monitoring, and surveillance, each tapping into its unique microwave imaging strengths. Here, I’ll focus on two applications demonstrating SAR’s versatility in addressing fundamentally different challenges.\n\n8.2.1 Multi-temporal SAR analysis\nFor environmental monitoring, multi-temporal SAR analysis enables reliable wildfire tracking where optical sensors fail due to smoke and darkness. Recent research by Ban et al. (2020) demonstrates how multi-temporal SAR analysis enables near real-time fire progression tracking using Sentinel-1 C-band data. Their approach combines log-ratio change detection of pre- and post-fire backscatter with deep learning (CNN) to automatically identify burn areas, achieving over 90% accuracy in some forested areas. They highlighted key challenges: burnt and dry grass yield similar C-band backscatter (complicating detection), and steep terrain causing false positives, which they addressed through multi-polarization analysis, L-band SAR integration, and optical data fusion. However, their validation framework raises concerns: training and validation datasets were randomly selected from the same geographical regions, risking spatial autocorrelation. This could inflate the reported 90% accuracy, as nearby pixels may share environmental context rather than reflect the CNN’s generalizable detection capability, potentially reducing performance in new regions with distinct terrain or vegetation.\n\n\n8.2.2 SAR-Based Oil Tank Measurement\nIn industrial infrastructure monitoring, SAR can provide precise measurement of Oil storage tanks. Ursa Space Systems employs Sentinel-1 C-band data to measure oil volume by the height of the floating lids in tanks, where higher lids indicate more oil quantity (Figure 3). As simple this approach seems, the blog highlights some challenges such as distinguishing tank signals from nearby infrastructure, handling signal interference from multiple adjacent tanks, and ensuring consistent imagery. They emphasize overcoming challenges like cloud cover, which hampers optical sensors, by sourcing consistent imagery from multiple SAR vendors.\n\n\n\nFigure 3 - Process of measurin oil volume . - Source: URSA Space",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperture Radar (SAR)</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#reflections",
    "href": "W9_SAR.html#reflections",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "8.3 Reflections",
    "text": "8.3 Reflections\n\nDuring the course, I was wondering why we were focusing more on optical sensors and weather SAR was mostly used for applications to solve correction issues like clouds for example. But now I realized how SAR’s unique capabilities extend far beyond “filling gaps.” Its ability to penetrate sand, smoke, and vegetation, while measuring millimeter-scale changes, offers insights optical sensors simply cannot provide.\nWhen I was writing the applications sections, I read a lot of papers from different domains just to see and expand my knowledge to see the diversity. An interesting paper was this, which highlighted how SAR images revealed buried ruins of Ubar city in Oman, emphasizing SAR’s ability to sand and uncover ancient subsurface structures invisible to optical sensors! Also, it’s interesting to see how researchers increasingly combine SAR and optical data to create more robust solutions. For example, optical sensors might classify land cover, while SAR detects subtle structural or moisture changes beneath the surface. This synergy mirrors a broader lesson: integrating diverse methods often yields richer results than relying on a single approach.\nBuilding on this, I’ll defiantly try to explore SAR’s potential for studying sandstorms, which is an interest I’ve touched upon before in this diary. As it’s a not a common global phenomena, there’s limited global research on it. sandstorms pose unique challenges: optical sensors fail during the event, while SAR’s penetration could track dust movement or subsurface impacts. By fusing SAR’s all-weather resilience with optical data’s intuitive clarity, I hope to develop a more comprehensive understanding of these events.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperture Radar (SAR)</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#references",
    "href": "W9_SAR.html#references",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "8.4 References",
    "text": "8.4 References\n\nBan, Y., Zhang, P., Nascetti, A. et al. Near Real-Time Wildfire Progression Monitoring with Sentinel-1 SAR Time Series and Deep Learning. Sci Rep 10, 1322 (2020). https://doi.org/10.1038/s41598-019-56967-x\nCampbell, B. A. (Bruce A. (2002). Radar remote sensing of planetary surfaces / Bruce A. Campbell. Cambridge University Press.\nGeoffrey Craig. (2020). An inside look at SAR-based measurements. URSA. Available at: https://ursaspace.com/blog/an-inside-look-at-sar-based-measurements/",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperture Radar (SAR)</span>"
    ]
  },
  {
    "objectID": "intro.html#summary",
    "href": "intro.html#summary",
    "title": "1  Introduction to Remote Sensing",
    "section": "",
    "text": "The first lecture of the module gave us an overview of remote sensing field, we discussed some advantages like huge data volumes, frequent updates, applications in policy-making, and the availability of free processing tools and some challenges such as the complexity of interpretation. Then we moved to more technical concepts that I’ll cover some here:\nSensors can be divided into two categories Passive and Active. Passive sensors rely on external energy sources (mostly sunlight) to detect reflected radiation. While cost-effective and capable of capturing detailed spectral information, passive systems are limited by daylight availability and atmospheric conditions like cloud cover. In contrast, active sensors generate their own energy, emitting radiation in the form of radar pulses or laser beams, they tend to be more complex and expensive to operate. Figure 1 summarizes how each category work visually.\n\n\n\nFigure 1 - Passive vs Active Remote Sensing. - Source\n\n\nBoth sensor types rely on electromagnetic radiation (EMR) interactions with the Earth’s surface, the way they interact with surfaces depends on three key factors: the wavelength of the radiation, the material properties of the surface (such as water, vegetation, or concrete), and the physical condition of that surface (like its roughness or moisture content). These interactions result in varying amounts of energy being absorbed, transmitted through, or reflected by the surface, which ultimately determines what the sensors detect and the quality of the remote sensing data we obtain.\nIn addition, remote sensing data is defined by four key resolutions: spatial (pixel size), temporal (how often observations are made), radiometric (range of detectable values), and spectral (bandwidth sensitivity across the electromagnetic spectrum). These parameters determine the detail, frequency, sensitivity, and wavelength-specific information captured by the sensor (Figure 2).\n\n\n\nFigure 2 - Resolutions categories. - Source\n\n\nIt’s interesting how technical trade-offs—higher spatial detail reducing coverage frequency, or richer spectral data demanding more processing—shape policy applications. Cloud-free optical data suits urban planning, but disaster response needs timely action, not perfect weather, making radar’s cloud-penetrating role vital. These constraints highlight why sensor choice matters: precision versus speed can influence effective decision-making,",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "intro.html#applications",
    "href": "intro.html#applications",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\n\nBuilding on my understanding of sensor types and resolution characteristics from this week’s lecture, I’ll explore how these technical specifications translate to real-world problem-solving within climate-related initiatives. Passive sensors, like those on Landsat 8, leverage sunlight to capture multispectral data across 11 spectral bands, enabling high spectral resolution (30m spatial resolution) that proves invaluable for long-term climate impact monitoring. For instance, Landsat’s thermal infrared band (Band 10) has been critical in tracking urban heat island effects with a 100m resolution, while its visible and near-infrared bands support vegetation health assessments through NDVI calculations (USGS, 2021). However, these capabilities come with inherent constraints - the 16-day revisit period (U.S. Geological Survey, 2023) and cloud sensitivity create temporal gaps that complicate real-time climate event monitoring.\nThis is where active sensor systems like GEDI (Global Ecosystem Dynamics Investigation) demonstrate their complementary value. Mounted on the ISS, GEDI’s lidar system penetrates vegetation canopies with laser pulses at 25m resolution, providing precise vertical structure data for carbon stock assessments that passive optical sensors cannot capture (Dubayah et al., 2020). The fusion of these datasets - combining Landsat’s spectral richness with GEDI’s structural measurements - exemplifies how modern climate science overcomes individual sensor limitations.\nThe technical trade-offs extend to temporal resolution considerations. While MODIS offers daily global coverage at 250m-1km resolution - ideal for tracking rapidly evolving phenomena like wildfire smoke plumes - its coarser spatial resolution limits detailed analysis. This has driven the development of hybrid solutions like ESA’s Sentinel-2 constellation, which provides 10-60m resolution with a 5-day revisit at mid-latitudes, significantly improving our ability to monitor deforestation fronts and glacier retreat (Drusch et al., 2012).\nThese applications underscore a main takeaway from our lecture: effective climate monitoring requires strategic sensor selection based on specific observational needs. The choice between high spectral resolution (Landsat), frequent temporal coverage (MODIS), or vertical profiling (GEDI) depends fundamentally on whether the research priority is long-term trend analysis, rapid event response, or 3D ecosystem modeling.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "intro.html#reflections",
    "href": "intro.html#reflections",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.3 Reflections",
    "text": "1.3 Reflections\n\nThe introductory lecture gave me a sense of what’s coming in the next few weeks, my initial thought was concern as many concepts seems complicated but the lecturer’s note that the field is “filled with jargon that often isn’t as complicated as it sounds” made me optimistic, I hope that I can say I agree with that statement by the end of this course!\nWhile I’ve previously worked with remote sensing products like temperature and elevation datasets, I never fully understood the underlying processes. Documentations were often overwhelming and complicated. Now, with these fundamental concepts, I feel equipped to not just use but truly understand and potentially recreate these analyses myself.\nThe lecture’s simple yet profound questions—“Why is the sky blue?”, “Why does the moon have a black sky?”, “Why is the ocean blue?”— highlighted how basic physical principles shape both what we observe in nature and what sensors detect technically. Now, I use these questions to simplify my answers when people ask what I’m studying and how satellites work, bridging everyday sights with the science behind them.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "intro.html#references",
    "href": "intro.html#references",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.4 References",
    "text": "1.4 References\n\nDrusch, M., Del Bello, U., Carlier, S., Colin, O., Fernandez, V., Gascon, F., Hoersch, B., Isola, C., Laberinti, P., Martimort, P., Meygret, A., Spoto, F., Sy, O., Marchese, F., & Bargellini, P. (2012). Sentinel-2: ESA’s Optical High-Resolution Mission for GMES Operational Services. Remote Sensing of Environment, 120, 25–36. https://doi.org/10.1016/j.rse.2011.11.026\nDubayah, R., Blair, J. B., Goetz, S., Fatoyinbo, L., Hansen, M., Healey, S., Hofton, M., Hurtt, G., Kellner, J., Luthcke, S., Armston, J., Tang, H., Duncanson, L., Hancock, S., Jantz, P., Marselis, S., Patterson, P. L., Qi, W., & Silva, C. (2020). The Global Ecosystem Dynamics Investigation: High-resolution laser ranging of the Earth’s forests and topography. Science of Remote Sensing, 1, 100002-. https://doi.org/10.1016/j.srs.2020.100002\nNASA. (n.d) MODIS. Available at: https://modis.gsfc.nasa.gov/data/\nU.S. Geological Survey. (2023). Landsat 9 data users handbook (Version 1.0). U.S. Department of the Interior. https://www.usgs.gov/media/files/landsat-9-data-users-handbook\nUSGS. (2021). Landsat 8-9 Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS) Level-2 Science Products. Version 2.0. U.S. Geological Survey. https://www.usgs.gov/core-science-systems/nli/landsat/landsat-collection-2-level-2-science-products",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Remote Sensing</span>"
    ]
  }
]